{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliregen/anaconda3/envs/py36/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "/Users/eliregen/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import regex as re\n",
    "from time import sleep\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, VotingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score, mean_absolute_error\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./roboBohr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['pubchem_id', 'Eat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -19.013763\n",
       "1   -10.161019\n",
       "2    -9.376619\n",
       "3   -13.776438\n",
       "4    -8.537140\n",
       "Name: Eat, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11369, 1275) (4873, 1275)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_sc = ss.transform(X_train)\n",
    "X_test_sc= ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3) 0.9602870228646436 0.8684481467178276\n",
      "(4, 4) 0.9869798322844361 0.6887213767821877\n",
      "(4, 5) 0.9870687837348018 0.5911146497985937\n",
      "(5, 3) 0.9785761278565747 0.7002133436284539\n",
      "(5, 4) 0.922302190687036 0.9788853319575508\n",
      "(5, 5) 0.9801312715645581 0.6511187704940113\n",
      "(6, 3) 0.9917049693042219 0.5592296994326075\n",
      "(6, 4) 0.3055919416497086 3.0462731314643\n",
      "(6, 5) 0.9857095226755858 0.6755547470606897\n",
      "(7, 3) 0.9738399952826781 0.7893722357442652\n",
      "(7, 4) 0.9852738140265526 0.6764545041264153\n",
      "(7, 5) 0.9899546227240249 0.551236877680121\n",
      "(8, 3) 0.9902193615744089 0.5723207035744949\n",
      "(8, 4) 0.962451514090024 0.8729869495906452\n",
      "(8, 5) 0.9921161152543864 0.6811194047991985\n"
     ]
    }
   ],
   "source": [
    "nn_models = {}\n",
    "for i in range(4,9):\n",
    "    for j in range(3,6):\n",
    "        nn_models[str((i,j))] = MLPRegressor((i,j), activation='relu', learning_rate='adaptive')\n",
    "        nn_models[str((i,j))].fit(X_train, y_train)\n",
    "        print((i,j), nn_models[str((i,j))].score(X_train, y_train), \n",
    "              np.sqrt(mean_squared_error(y_test, nn_models[str((i,j))].predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid = pd.read_csv('xgboost_grid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_eval_metric</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>...</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294.038347</td>\n",
       "      <td>11.553681</td>\n",
       "      <td>0.507512</td>\n",
       "      <td>0.084625</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999059</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.999164</td>\n",
       "      <td>0.999183</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>459.409907</td>\n",
       "      <td>14.336957</td>\n",
       "      <td>0.718571</td>\n",
       "      <td>0.288475</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.998993</td>\n",
       "      <td>0.999276</td>\n",
       "      <td>0.999336</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999249</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2457.968370</td>\n",
       "      <td>1765.243059</td>\n",
       "      <td>0.473443</td>\n",
       "      <td>0.127449</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999061</td>\n",
       "      <td>0.999162</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.999047</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1248.653586</td>\n",
       "      <td>1429.168859</td>\n",
       "      <td>0.719830</td>\n",
       "      <td>0.217461</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.998981</td>\n",
       "      <td>0.999276</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>0.999373</td>\n",
       "      <td>0.999247</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>381.401560</td>\n",
       "      <td>5.582550</td>\n",
       "      <td>0.604744</td>\n",
       "      <td>0.137968</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999054</td>\n",
       "      <td>0.998775</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>0.999162</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>633.996257</td>\n",
       "      <td>62.220302</td>\n",
       "      <td>0.848202</td>\n",
       "      <td>0.274835</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999259</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>482.592856</td>\n",
       "      <td>44.507498</td>\n",
       "      <td>0.842158</td>\n",
       "      <td>0.157051</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998928</td>\n",
       "      <td>0.998586</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>0.999118</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>0.998949</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>563.142867</td>\n",
       "      <td>38.154015</td>\n",
       "      <td>0.617772</td>\n",
       "      <td>0.035911</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>0.998731</td>\n",
       "      <td>0.999140</td>\n",
       "      <td>0.999233</td>\n",
       "      <td>0.999247</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>330.876423</td>\n",
       "      <td>2.234285</td>\n",
       "      <td>0.439036</td>\n",
       "      <td>0.023050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.999122</td>\n",
       "      <td>0.999121</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>491.885414</td>\n",
       "      <td>0.592714</td>\n",
       "      <td>0.606554</td>\n",
       "      <td>0.056945</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999060</td>\n",
       "      <td>0.998763</td>\n",
       "      <td>0.999146</td>\n",
       "      <td>0.999239</td>\n",
       "      <td>0.999253</td>\n",
       "      <td>0.999092</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>330.052423</td>\n",
       "      <td>4.114967</td>\n",
       "      <td>0.465331</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.998604</td>\n",
       "      <td>0.998999</td>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.999139</td>\n",
       "      <td>0.998961</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>494.463806</td>\n",
       "      <td>3.206682</td>\n",
       "      <td>0.621482</td>\n",
       "      <td>0.052809</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999064</td>\n",
       "      <td>0.998774</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.999240</td>\n",
       "      <td>0.999273</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>375.265429</td>\n",
       "      <td>2.609677</td>\n",
       "      <td>0.674153</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998762</td>\n",
       "      <td>0.998399</td>\n",
       "      <td>0.998871</td>\n",
       "      <td>0.999007</td>\n",
       "      <td>0.999030</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>554.485405</td>\n",
       "      <td>1.030954</td>\n",
       "      <td>0.787997</td>\n",
       "      <td>0.045235</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998854</td>\n",
       "      <td>0.998527</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.999121</td>\n",
       "      <td>0.998920</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>375.076704</td>\n",
       "      <td>2.958487</td>\n",
       "      <td>0.654361</td>\n",
       "      <td>0.041125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998783</td>\n",
       "      <td>0.998417</td>\n",
       "      <td>0.998896</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>0.999060</td>\n",
       "      <td>0.998836</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>619.806951</td>\n",
       "      <td>66.238345</td>\n",
       "      <td>0.993250</td>\n",
       "      <td>0.185130</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998883</td>\n",
       "      <td>0.998556</td>\n",
       "      <td>0.999032</td>\n",
       "      <td>0.999121</td>\n",
       "      <td>0.999158</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>477.291617</td>\n",
       "      <td>22.068189</td>\n",
       "      <td>0.871904</td>\n",
       "      <td>0.380060</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998810</td>\n",
       "      <td>0.998443</td>\n",
       "      <td>0.998908</td>\n",
       "      <td>0.999022</td>\n",
       "      <td>0.999083</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>575.631354</td>\n",
       "      <td>31.309450</td>\n",
       "      <td>0.792284</td>\n",
       "      <td>0.035457</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998907</td>\n",
       "      <td>0.998594</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.999120</td>\n",
       "      <td>0.999183</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>285.671188</td>\n",
       "      <td>1.049115</td>\n",
       "      <td>0.383349</td>\n",
       "      <td>0.029922</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.999312</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>0.999441</td>\n",
       "      <td>0.999318</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>427.185297</td>\n",
       "      <td>1.289411</td>\n",
       "      <td>0.432289</td>\n",
       "      <td>0.025971</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.999336</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>286.946171</td>\n",
       "      <td>0.807684</td>\n",
       "      <td>0.360214</td>\n",
       "      <td>0.053518</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.999082</td>\n",
       "      <td>0.999325</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999440</td>\n",
       "      <td>0.999322</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>430.321831</td>\n",
       "      <td>0.814387</td>\n",
       "      <td>0.405814</td>\n",
       "      <td>0.030785</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999107</td>\n",
       "      <td>0.999349</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.999350</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>285.724265</td>\n",
       "      <td>0.500732</td>\n",
       "      <td>0.375832</td>\n",
       "      <td>0.070387</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999343</td>\n",
       "      <td>0.999107</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>461.365637</td>\n",
       "      <td>29.636740</td>\n",
       "      <td>0.490312</td>\n",
       "      <td>0.078667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.999137</td>\n",
       "      <td>0.999369</td>\n",
       "      <td>0.999437</td>\n",
       "      <td>0.999476</td>\n",
       "      <td>0.999359</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>406.533167</td>\n",
       "      <td>17.682693</td>\n",
       "      <td>0.574332</td>\n",
       "      <td>0.142315</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.999283</td>\n",
       "      <td>0.999109</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>540.226937</td>\n",
       "      <td>19.911622</td>\n",
       "      <td>0.639469</td>\n",
       "      <td>0.046978</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999092</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.999169</td>\n",
       "      <td>0.999264</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.999122</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>386.627945</td>\n",
       "      <td>22.259621</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>0.068557</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999076</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.999186</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.999128</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>512.785677</td>\n",
       "      <td>4.501507</td>\n",
       "      <td>0.492925</td>\n",
       "      <td>0.020385</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999096</td>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.999307</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>328.762438</td>\n",
       "      <td>4.248158</td>\n",
       "      <td>0.409044</td>\n",
       "      <td>0.073203</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999094</td>\n",
       "      <td>0.998823</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>0.999281</td>\n",
       "      <td>0.999290</td>\n",
       "      <td>0.999139</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>509.790008</td>\n",
       "      <td>38.260423</td>\n",
       "      <td>0.590737</td>\n",
       "      <td>0.142168</td>\n",
       "      <td>0.2</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.2, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999115</td>\n",
       "      <td>0.998841</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.999295</td>\n",
       "      <td>0.999302</td>\n",
       "      <td>0.999155</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>477.556661</td>\n",
       "      <td>2.212798</td>\n",
       "      <td>0.504028</td>\n",
       "      <td>0.073048</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.998637</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.999128</td>\n",
       "      <td>0.999133</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>711.413985</td>\n",
       "      <td>2.946320</td>\n",
       "      <td>0.539982</td>\n",
       "      <td>0.039746</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.999245</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>475.508847</td>\n",
       "      <td>4.547943</td>\n",
       "      <td>0.474806</td>\n",
       "      <td>0.021666</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.998630</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.999139</td>\n",
       "      <td>0.998972</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>715.606283</td>\n",
       "      <td>2.379030</td>\n",
       "      <td>0.566607</td>\n",
       "      <td>0.020735</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.998759</td>\n",
       "      <td>0.999137</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.999088</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>474.943318</td>\n",
       "      <td>3.439280</td>\n",
       "      <td>0.445883</td>\n",
       "      <td>0.033368</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.998653</td>\n",
       "      <td>0.999019</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>0.999140</td>\n",
       "      <td>0.998979</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>709.973897</td>\n",
       "      <td>2.519229</td>\n",
       "      <td>0.533732</td>\n",
       "      <td>0.020765</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.998792</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>0.999218</td>\n",
       "      <td>0.999253</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>556.655898</td>\n",
       "      <td>1.515757</td>\n",
       "      <td>0.705671</td>\n",
       "      <td>0.101034</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998783</td>\n",
       "      <td>0.998457</td>\n",
       "      <td>0.998924</td>\n",
       "      <td>0.999018</td>\n",
       "      <td>0.999059</td>\n",
       "      <td>0.998848</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>830.074280</td>\n",
       "      <td>2.339072</td>\n",
       "      <td>0.747501</td>\n",
       "      <td>0.089426</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.999134</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>555.556106</td>\n",
       "      <td>1.585774</td>\n",
       "      <td>0.576864</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998816</td>\n",
       "      <td>0.998476</td>\n",
       "      <td>0.998935</td>\n",
       "      <td>0.999041</td>\n",
       "      <td>0.999093</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>831.969347</td>\n",
       "      <td>1.207717</td>\n",
       "      <td>0.825615</td>\n",
       "      <td>0.161867</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998891</td>\n",
       "      <td>0.998592</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.999174</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>555.531522</td>\n",
       "      <td>2.731756</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>0.073156</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998829</td>\n",
       "      <td>0.998511</td>\n",
       "      <td>0.998944</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.999094</td>\n",
       "      <td>0.998883</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>830.082234</td>\n",
       "      <td>2.301159</td>\n",
       "      <td>0.731502</td>\n",
       "      <td>0.044621</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998910</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.999124</td>\n",
       "      <td>0.999178</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>399.407287</td>\n",
       "      <td>4.372924</td>\n",
       "      <td>0.348348</td>\n",
       "      <td>0.043249</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999305</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999404</td>\n",
       "      <td>0.999289</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>591.561690</td>\n",
       "      <td>1.250974</td>\n",
       "      <td>0.403436</td>\n",
       "      <td>0.030712</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999334</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.999395</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>0.999317</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>394.152640</td>\n",
       "      <td>1.606339</td>\n",
       "      <td>0.307152</td>\n",
       "      <td>0.028526</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999294</td>\n",
       "      <td>0.999059</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.999387</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.999299</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>590.216977</td>\n",
       "      <td>0.622315</td>\n",
       "      <td>0.386698</td>\n",
       "      <td>0.028688</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999325</td>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>394.111911</td>\n",
       "      <td>0.513892</td>\n",
       "      <td>0.360001</td>\n",
       "      <td>0.055458</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>0.999365</td>\n",
       "      <td>0.999413</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>592.304242</td>\n",
       "      <td>2.797218</td>\n",
       "      <td>0.367569</td>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.999441</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>480.964454</td>\n",
       "      <td>1.524732</td>\n",
       "      <td>0.391083</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.998759</td>\n",
       "      <td>0.999179</td>\n",
       "      <td>0.999233</td>\n",
       "      <td>0.999266</td>\n",
       "      <td>0.999101</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>714.617375</td>\n",
       "      <td>2.598647</td>\n",
       "      <td>0.457596</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999084</td>\n",
       "      <td>0.998771</td>\n",
       "      <td>0.999190</td>\n",
       "      <td>0.999243</td>\n",
       "      <td>0.999277</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>474.318070</td>\n",
       "      <td>1.132637</td>\n",
       "      <td>0.342768</td>\n",
       "      <td>0.021876</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999082</td>\n",
       "      <td>0.998786</td>\n",
       "      <td>0.999169</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>0.999290</td>\n",
       "      <td>0.999110</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>711.369972</td>\n",
       "      <td>1.001421</td>\n",
       "      <td>0.474756</td>\n",
       "      <td>0.038797</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.999183</td>\n",
       "      <td>0.999235</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>474.137526</td>\n",
       "      <td>0.849596</td>\n",
       "      <td>0.350845</td>\n",
       "      <td>0.060979</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.998822</td>\n",
       "      <td>0.999176</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>713.472835</td>\n",
       "      <td>2.034714</td>\n",
       "      <td>0.435243</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.998839</td>\n",
       "      <td>0.999191</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999133</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>555.645186</td>\n",
       "      <td>0.975256</td>\n",
       "      <td>0.446444</td>\n",
       "      <td>0.022738</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998879</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>0.999045</td>\n",
       "      <td>0.999086</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>0.998931</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>831.666543</td>\n",
       "      <td>1.866135</td>\n",
       "      <td>0.508351</td>\n",
       "      <td>0.036502</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998890</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.999093</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>552.918752</td>\n",
       "      <td>1.268276</td>\n",
       "      <td>0.433215</td>\n",
       "      <td>0.094404</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.998609</td>\n",
       "      <td>0.999058</td>\n",
       "      <td>0.999115</td>\n",
       "      <td>0.999142</td>\n",
       "      <td>0.998965</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>832.733391</td>\n",
       "      <td>1.970639</td>\n",
       "      <td>0.513204</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999124</td>\n",
       "      <td>0.999149</td>\n",
       "      <td>0.998974</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>552.488146</td>\n",
       "      <td>0.832119</td>\n",
       "      <td>0.428769</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998923</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.999090</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>0.999184</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>762.737787</td>\n",
       "      <td>92.941871</td>\n",
       "      <td>0.443652</td>\n",
       "      <td>0.075032</td>\n",
       "      <td>0.4</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'colsample_bytree': 0.4, 'eval_metric': 'rmse...</td>\n",
       "      <td>0.998937</td>\n",
       "      <td>0.998679</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.999007</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      294.038347     11.553681         0.507512        0.084625   \n",
       "1      459.409907     14.336957         0.718571        0.288475   \n",
       "2     2457.968370   1765.243059         0.473443        0.127449   \n",
       "3     1248.653586   1429.168859         0.719830        0.217461   \n",
       "4      381.401560      5.582550         0.604744        0.137968   \n",
       "..            ...           ...              ...             ...   \n",
       "67     831.666543      1.866135         0.508351        0.036502   \n",
       "68     552.918752      1.268276         0.433215        0.094404   \n",
       "69     832.733391      1.970639         0.513204        0.041185   \n",
       "70     552.488146      0.832119         0.428769        0.072051   \n",
       "71     762.737787     92.941871         0.443652        0.075032   \n",
       "\n",
       "    param_colsample_bytree param_eval_metric  param_learning_rate  \\\n",
       "0                      0.2              rmse               0.0156   \n",
       "1                      0.2              rmse               0.0156   \n",
       "2                      0.2              rmse               0.0156   \n",
       "3                      0.2              rmse               0.0156   \n",
       "4                      0.2              rmse               0.0156   \n",
       "..                     ...               ...                  ...   \n",
       "67                     0.4              rmse               0.0625   \n",
       "68                     0.4              rmse               0.0625   \n",
       "69                     0.4              rmse               0.0625   \n",
       "70                     0.4              rmse               0.0625   \n",
       "71                     0.4              rmse               0.0625   \n",
       "\n",
       "    param_max_depth  param_min_child_weight  param_n_estimators  ...  \\\n",
       "0                 6                       9                 400  ...   \n",
       "1                 6                       9                 600  ...   \n",
       "2                 6                      10                 400  ...   \n",
       "3                 6                      10                 600  ...   \n",
       "4                 6                      11                 400  ...   \n",
       "..              ...                     ...                 ...  ...   \n",
       "67               10                       9                 600  ...   \n",
       "68               10                      10                 400  ...   \n",
       "69               10                      10                 600  ...   \n",
       "70               10                      11                 400  ...   \n",
       "71               10                      11                 600  ...   \n",
       "\n",
       "   param_reg_lambda                                             params  \\\n",
       "0                 0  {'colsample_bytree': 0.2, 'eval_metric': 'rmse...   \n",
       "1                 0  {'colsample_bytree': 0.2, 'eval_metric': 'rmse...   \n",
       "2                 0  {'colsample_bytree': 0.2, 'eval_metric': 'rmse...   \n",
       "3                 0  {'colsample_bytree': 0.2, 'eval_metric': 'rmse...   \n",
       "4                 0  {'colsample_bytree': 0.2, 'eval_metric': 'rmse...   \n",
       "..              ...                                                ...   \n",
       "67                0  {'colsample_bytree': 0.4, 'eval_metric': 'rmse...   \n",
       "68                0  {'colsample_bytree': 0.4, 'eval_metric': 'rmse...   \n",
       "69                0  {'colsample_bytree': 0.4, 'eval_metric': 'rmse...   \n",
       "70                0  {'colsample_bytree': 0.4, 'eval_metric': 'rmse...   \n",
       "71                0  {'colsample_bytree': 0.4, 'eval_metric': 'rmse...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.999059           0.998777           0.999063           0.999164   \n",
       "1           0.999268           0.998993           0.999276           0.999336   \n",
       "2           0.999057           0.998768           0.999061           0.999162   \n",
       "3           0.999265           0.998981           0.999276           0.999339   \n",
       "4           0.999054           0.998775           0.999062           0.999162   \n",
       "..               ...                ...                ...                ...   \n",
       "67          0.998890           0.998550           0.999055           0.999093   \n",
       "68          0.998900           0.998609           0.999058           0.999115   \n",
       "69          0.998913           0.998618           0.999068           0.999124   \n",
       "70          0.998923           0.998667           0.999090           0.999119   \n",
       "71          0.998937           0.998679           0.999102           0.999125   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.999183         0.999049        0.000145               40  \n",
       "1            0.999370         0.999249        0.000133               14  \n",
       "2            0.999189         0.999047        0.000149               42  \n",
       "3            0.999373         0.999247        0.000139               15  \n",
       "4            0.999189         0.999049        0.000147               41  \n",
       "..                ...              ...             ...              ...  \n",
       "67           0.999105         0.998939        0.000209               61  \n",
       "68           0.999142         0.998965        0.000197               55  \n",
       "69           0.999149         0.998974        0.000196               49  \n",
       "70           0.999184         0.998997        0.000186               45  \n",
       "71           0.999194         0.999007        0.000185               43  \n",
       "\n",
       "[72 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=X.shape[1],kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(500,kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(45,kernel_initializer='normal'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-2, patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11369 samples, validate on 4873 samples\n",
      "Epoch 1/20\n",
      "11369/11369 [==============================] - 100s 9ms/step - loss: 2.2747 - val_loss: 0.2253\n",
      "Epoch 2/20\n",
      "11369/11369 [==============================] - 45s 4ms/step - loss: 0.4587 - val_loss: 0.1167\n",
      "Epoch 3/20\n",
      "11369/11369 [==============================] - 39s 3ms/step - loss: 0.3612 - val_loss: 0.1317\n",
      "Epoch 4/20\n",
      "11369/11369 [==============================] - 54s 5ms/step - loss: 0.3732 - val_loss: 0.3175\n",
      "Epoch 5/20\n",
      "11369/11369 [==============================] - 47s 4ms/step - loss: 0.3626 - val_loss: 0.2804\n",
      "Epoch 6/20\n",
      "11369/11369 [==============================] - 44s 4ms/step - loss: 0.3630 - val_loss: 0.1382\n",
      "Epoch 7/20\n",
      "11369/11369 [==============================] - 66s 6ms/step - loss: 0.3525 - val_loss: 0.1071\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,validation_data=(X_test,y_test),callbacks=[monitor],verbose=1,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_fun(predicted, actual):\n",
    "    return np.sqrt(mean_squared_error(y_test, hist_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32730833377367125"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_fun(y_test, hist_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-14.62808  ],\n",
       "       [-14.361696 ],\n",
       "       [-12.678047 ],\n",
       "       ...,\n",
       "       [-10.266216 ],\n",
       "       [-17.202717 ],\n",
       "       [ -7.6958346]], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-12.375125 , -14.9746065, -15.711248 , ..., -10.114859 ,\n",
       "       -11.7536335,  -7.537242 ], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_pred.reshape((4873,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32730833377367125"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_fun(y_test, hist_pred.reshape((4873,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.2, eval_metric='rmse',\n",
    "             gamma=0, importance_type='gain', learning_rate = 0.0625,\n",
    "             max_delta_step=0, max_depth = 5, min_child_weight = 9, missing=None,\n",
    "             n_estimators=3000, n_jobs=-1, nthread=None,\n",
    "             objective='reg:squarederror', random_state=42, reg_alpha=0,\n",
    "             reg_lambda=0, scale_pos_weight=1, seed=None, silent=None,\n",
    "             subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error for xgb model is 0.07274142128838246\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(X_train, y_train, verbose=1)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"The error for xgb model is \" + str(np.sqrt(mean_squared_error(y_test, y_pred_xgb))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07295909287291316"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.07295909287291316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008114347921282534"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_train, xgb_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010392171573046807"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.010392171573046807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model2 = XGBRegressor(objective='reg:linear', eval_metric = 'rmse', learning_rate = 0.0625, reg_lambda = 0,\n",
    "                         max_depth = 6, colsample_bytree = 0.2, min_child_weight = 11, n_estimators = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = lgbm_test = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.5,\n",
    "              importance_type='split', learning_rate=0.1, max_depth=5,\n",
    "              min_child_samples=26, min_child_weight=4, min_split_gain=0.0,\n",
    "              n_estimators=2500, n_jobs=2, num_leaves =22,\n",
    "              objective=None, random_state=42, reg_alpha=0.0, reg_lambda=0.0,\n",
    "              silent=True, subsample=1.0, subsample_for_bin=200000,\n",
    "              subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error for lgb model is 0.07824556296135753\n"
     ]
    }
   ],
   "source": [
    "lgbm_model.fit(X_train, y_train, verbose=1)\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "print(\"The error for lgb model is \" + str(np.sqrt(mean_squared_error(y_test, y_pred_lgbm))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07824556296135753"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.07824556296135753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016427869058813573"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_train, lgbm_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02778048784340118"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.02778048784340118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=80, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=3, n_neighbors=3, p=1,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3,\n",
    "                         weights='uniform',\n",
    "                          algorithm='auto',\n",
    "                          leaf_size=80,\n",
    "                          p=1,\n",
    "                          metric='minkowski',\n",
    "                          metric_params=None,\n",
    "                          n_jobs=3,)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error for knn model is 0.26687925312758193\n"
     ]
    }
   ],
   "source": [
    "knn_pred = knn.predict(X_test)\n",
    "print(\"The error for knn model is \" + str(np.sqrt(mean_squared_error(y_test, knn_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27415052551252345"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.27415052551252345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17741826585282713"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_train, knn.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14787086044700148"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.14787086044700148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 51.8min\n",
      "[Parallel(n_jobs=2)]: Done 135 out of 135 | elapsed: 186.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seconds 11211.977\n",
      "Best score and parameter combination = \n",
      "-0.14002564179226554\n",
      "{'knn__leaf_size': 80, 'knn__metric': 'minkowski', 'knn__n_neighbors': 2, 'knn__p': 1, 'knn__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])\n",
    "'distance'\n",
    "param_dist = {\"knn__n_neighbors\": [2, 3, 5],\n",
    "              \"knn__weights\": ['uniform'],\n",
    "              \"knn__leaf_size\": [80, 90, 100],\n",
    "              \"knn__p\":[1, 2, 3],\n",
    "              \"knn__metric\":['minkowski']\n",
    "             }\n",
    "#, 'distance'\n",
    "CV = GridSearchCV(pipe, param_grid = param_dist, scoring = 'neg_mean_absolute_error', n_jobs= 2, cv=5, verbose=1)\n",
    "CV.fit(X, y)  \n",
    "print(\" Seconds %0.3f\" % (time.time() - start_time))\n",
    "print('Best score and parameter combination = ')\n",
    "\n",
    "print(CV.best_score_)    \n",
    "print(CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = make_scorer(rmse_fun, greater_is_better=False)\n",
    "#let's see how long this all takes\n",
    "start_time = time.time()\n",
    "models = [\n",
    "     ('Stacked', stacked_model),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:06:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:09:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:13:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:18:03] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:21:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:25:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:30:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:33:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:36:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:40:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:46:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:51:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:56:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[10:59:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:03:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:06:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:10:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:14:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:19:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:22:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:25:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:31:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:34:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:37:47] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:41:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:46:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:56:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:00:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:08:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      " Seconds 7867.421\n"
     ]
    }
   ],
   "source": [
    "scores = [\n",
    "    -1.0 * cross_val_score(model, X_train.values, y_train.values, scoring=rmse, cv=5).mean()\n",
    "    for _,model in models\n",
    "]\n",
    "print(\" Seconds %0.3f\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27625960815911127]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:17:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:21:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:26:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:33:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:42:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:50:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingCVRegressor(cv=5,\n",
       "                    meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
       "                                         fit_intercept=True, max_iter=None,\n",
       "                                         normalize=False, random_state=None,\n",
       "                                         solver='auto', tol=0.001),\n",
       "                    n_jobs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "                    refit=True,\n",
       "                    regressors=[XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                             colsample_bylevel=1,\n",
       "                                             colsample_bynode=1,\n",
       "                                             colsample_bytree=0.2,\n",
       "                                             eval_met...\n",
       "                                              learning_rate=0.08, max_depth=4,\n",
       "                                              min_child_samples=20,\n",
       "                                              min_child_weight=5,\n",
       "                                              min_split_gain=0.0,\n",
       "                                              n_estimators=250, n_jobs=-1,\n",
       "                                              num_leaves=25, objective=None,\n",
       "                                              random_state=None, reg_alpha=0.0,\n",
       "                                              reg_lambda=0.0, silent=True,\n",
       "                                              subsample=1.0,\n",
       "                                              subsample_for_bin=200000,\n",
       "                                              subsample_freq=0)],\n",
       "                    shuffle=True, store_train_meta_features=False,\n",
       "                    use_features_in_secondary=False, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model4 = XGBRegressor(objective='reg:linear', eval_metric = 'rmse', learning_rate = 0.0625, reg_lambda = 0.1,\n",
    "                         max_depth = 6, colsample_bytree = 0.2, min_child_weight = 11, n_estimators = 600)\n",
    "\n",
    "lgbm2_model = LGBMRegressor(num_leaves = 25, n_estimators = 250, min_child_weight = 5, max_depth = 4, learning_rate = 0.08,\n",
    "                           colsample_bytree = 0.3)\n",
    "\n",
    "\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliregen/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for stacked models is 0.08173451712261762\n"
     ]
    }
   ],
   "source": [
    "y_pred_stack = stacked_model.predict(X_test.as_matrix())\n",
    "print(\"The score for stacked models is \"+ str(np.sqrt(mean_squared_error(y_test, y_pred_stack))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstacked2_model = StackingCVRegressor(\n",
    "    regressors=[xgb_model5, lgbm3_model],\n",
    "    meta_regressor=Ridge()\n",
    ")\n",
    "xgb_model5 = XGBRegressor(objective='reg:linear', eval_metric = 'rmse', learning_rate = 0.01, reg_lambda = 0.1,\n",
    "                         max_depth = 6, colsample_bytree = 0.2, min_child_weight = 11, n_estimators = 600)\n",
    "\n",
    "lgbm3_model = LGBMRegressor(num_leaves = 25, n_estimators = 600, min_child_weight = 5, max_depth = 4, learning_rate = 0.01,\n",
    "                           colsample_bytree = 0.5)\n",
    "\n",
    "\n",
    "stacked2_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stacked2_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e5b8cfb1f98e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstack2_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked2_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The score for stacked models is \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack2_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stacked2_model' is not defined"
     ]
    }
   ],
   "source": [
    "stack2_pred = stacked2_model.predict(X_test)\n",
    "print(\"The score for stacked models is \"+ str(np.sqrt(mean_squared_error(y_test, stack2_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, stacked2_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11369, 1275) (4873, 1275)\n"
     ]
    }
   ],
   "source": [
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_basemodel = XGBRegressor(objective='reg:squarederror', eval_metric = 'rmse', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_basemodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_base_pred = xgb_basemodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test, xgb_base_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_basemodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, xgb_basemodel.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "def modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'])\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain['Disbursed'], cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.2,\n",
       " 'eval_metric': 'rmse',\n",
       " 'learning_rate': 0.0625,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 11,\n",
       " 'n_estimators': 700,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'reg_lambda': 0}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gridsearcher.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 68.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seconds 8343.627\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xgb_parameters = {'objective':['reg:squarederror'],\n",
    "    'eval_metric':['rmse'],\n",
    "    'learning_rate':[0.0625],\n",
    "    'reg_lambda' :[0],\n",
    "    'max_depth' :[6],\n",
    "    'colsample_bytree':[0.2],\n",
    "    'min_child_weight':[8, 9, 10, 11],\n",
    "    'n_estimators':[1000]\n",
    "}\n",
    "\n",
    "xgb_gridsearcher = GridSearchCV(XGBRegressor(), xgb_parameters, verbose=1, n_jobs=-1, cv=5)\n",
    "xgb_gridsearcher.fit(X_train, y_train)\n",
    "print(\" Seconds %0.3f\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.2,\n",
       " 'eval_metric': 'rmse',\n",
       " 'learning_rate': 0.0625,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 9,\n",
       " 'n_estimators': 1000,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'reg_lambda': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gridsearcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 2000building tree 2 of 2000\n",
      "\n",
      "building tree 3 of 2000\n",
      "building tree 4 of 2000\n",
      "building tree 5 of 2000\n",
      "building tree 6 of 2000\n",
      "building tree 7 of 2000\n",
      "building tree 8 of 2000\n",
      "building tree 9 of 2000\n",
      "building tree 10 of 2000\n",
      "building tree 11 of 2000\n",
      "building tree 12 of 2000\n",
      "building tree 13 of 2000\n",
      "building tree 14 of 2000\n",
      "building tree 15 of 2000\n",
      "building tree 16 of 2000\n",
      "building tree 17 of 2000\n",
      "building tree 18 of 2000\n",
      "building tree 19 of 2000\n",
      "building tree 20 of 2000\n",
      "building tree 21 of 2000\n",
      "building tree 22 of 2000\n",
      "building tree 23 of 2000\n",
      "building tree 24 of 2000\n",
      "building tree 25 of 2000\n",
      "building tree 26 of 2000\n",
      "building tree 27 of 2000\n",
      "building tree 28 of 2000\n",
      "building tree 29 of 2000\n",
      "building tree 30 of 2000\n",
      "building tree 31 of 2000\n",
      "building tree 32 of 2000\n",
      "building tree 33 of 2000\n",
      "building tree 34 of 2000\n",
      "building tree 35 of 2000\n",
      "building tree 36 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 37 of 2000\n",
      "building tree 38 of 2000\n",
      "building tree 39 of 2000\n",
      "building tree 40 of 2000\n",
      "building tree 41 of 2000\n",
      "building tree 42 of 2000\n",
      "building tree 43 of 2000\n",
      "building tree 44 of 2000\n",
      "building tree 45 of 2000\n",
      "building tree 46 of 2000\n",
      "building tree 47 of 2000\n",
      "building tree 48 of 2000\n",
      "building tree 49 of 2000\n",
      "building tree 50 of 2000\n",
      "building tree 51 of 2000\n",
      "building tree 52 of 2000\n",
      "building tree 53 of 2000\n",
      "building tree 54 of 2000\n",
      "building tree 55 of 2000\n",
      "building tree 56 of 2000\n",
      "building tree 57 of 2000\n",
      "building tree 58 of 2000\n",
      "building tree 59 of 2000\n",
      "building tree 60 of 2000\n",
      "building tree 61 of 2000\n",
      "building tree 62 of 2000\n",
      "building tree 63 of 2000\n",
      "building tree 64 of 2000\n",
      "building tree 65 of 2000\n",
      "building tree 66 of 2000\n",
      "building tree 67 of 2000\n",
      "building tree 68 of 2000\n",
      "building tree 69 of 2000building tree 70 of 2000\n",
      "\n",
      "building tree 71 of 2000\n",
      "building tree 72 of 2000\n",
      "building tree 73 of 2000\n",
      "building tree 74 of 2000\n",
      "building tree 75 of 2000\n",
      "building tree 76 of 2000\n",
      "building tree 77 of 2000\n",
      "building tree 78 of 2000\n",
      "building tree 79 of 2000\n",
      "building tree 80 of 2000\n",
      "building tree 81 of 2000\n",
      "building tree 82 of 2000\n",
      "building tree 83 of 2000\n",
      "building tree 84 of 2000\n",
      "building tree 85 of 2000\n",
      "building tree 86 of 2000\n",
      "building tree 87 of 2000\n",
      "building tree 88 of 2000\n",
      "building tree 89 of 2000\n",
      "building tree 90 of 2000\n",
      "building tree 91 of 2000\n",
      "building tree 92 of 2000\n",
      "building tree 93 of 2000\n",
      "building tree 94 of 2000\n",
      "building tree 95 of 2000\n",
      "building tree 96 of 2000\n",
      "building tree 97 of 2000\n",
      "building tree 98 of 2000\n",
      "building tree 99 of 2000\n",
      "building tree 100 of 2000\n",
      "building tree 101 of 2000\n",
      "building tree 102 of 2000\n",
      "building tree 103 of 2000\n",
      "building tree 104 of 2000\n",
      "building tree 105 of 2000\n",
      "building tree 106 of 2000\n",
      "building tree 107 of 2000\n",
      "building tree 108 of 2000\n",
      "building tree 109 of 2000\n",
      "building tree 110 of 2000\n",
      "building tree 111 of 2000\n",
      "building tree 112 of 2000\n",
      "building tree 113 of 2000\n",
      "building tree 114 of 2000\n",
      "building tree 115 of 2000\n",
      "building tree 116 of 2000\n",
      "building tree 117 of 2000\n",
      "building tree 118 of 2000\n",
      "building tree 119 of 2000\n",
      "building tree 120 of 2000\n",
      "building tree 121 of 2000\n",
      "building tree 122 of 2000\n",
      "building tree 123 of 2000\n",
      "building tree 124 of 2000\n",
      "building tree 125 of 2000\n",
      "building tree 126 of 2000\n",
      "building tree 127 of 2000\n",
      "building tree 128 of 2000\n",
      "building tree 129 of 2000\n",
      "building tree 130 of 2000\n",
      "building tree 131 of 2000\n",
      "building tree 132 of 2000\n",
      "building tree 133 of 2000\n",
      "building tree 134 of 2000\n",
      "building tree 135 of 2000\n",
      "building tree 136 of 2000\n",
      "building tree 137 of 2000\n",
      "building tree 138 of 2000\n",
      "building tree 139 of 2000\n",
      "building tree 140 of 2000\n",
      "building tree 141 of 2000\n",
      "building tree 142 of 2000\n",
      "building tree 143 of 2000\n",
      "building tree 144 of 2000\n",
      "building tree 145 of 2000\n",
      "building tree 146 of 2000\n",
      "building tree 147 of 2000\n",
      "building tree 148 of 2000\n",
      "building tree 149 of 2000\n",
      "building tree 150 of 2000\n",
      "building tree 151 of 2000\n",
      "building tree 152 of 2000\n",
      "building tree 153 of 2000\n",
      "building tree 154 of 2000\n",
      "building tree 155 of 2000\n",
      "building tree 156 of 2000\n",
      "building tree 157 of 2000\n",
      "building tree 158 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 159 of 2000\n",
      "building tree 160 of 2000\n",
      "building tree 161 of 2000\n",
      "building tree 162 of 2000\n",
      "building tree 163 of 2000\n",
      "building tree 164 of 2000\n",
      "building tree 165 of 2000\n",
      "building tree 166 of 2000\n",
      "building tree 167 of 2000\n",
      "building tree 168 of 2000\n",
      "building tree 169 of 2000\n",
      "building tree 170 of 2000\n",
      "building tree 171 of 2000\n",
      "building tree 172 of 2000\n",
      "building tree 173 of 2000\n",
      "building tree 174 of 2000\n",
      "building tree 175 of 2000\n",
      "building tree 176 of 2000\n",
      "building tree 177 of 2000\n",
      "building tree 178 of 2000\n",
      "building tree 179 of 2000\n",
      "building tree 180 of 2000\n",
      "building tree 181 of 2000\n",
      "building tree 182 of 2000\n",
      "building tree 183 of 2000\n",
      "building tree 184 of 2000\n",
      "building tree 185 of 2000\n",
      "building tree 186 of 2000\n",
      "building tree 187 of 2000\n",
      "building tree 188 of 2000\n",
      "building tree 189 of 2000\n",
      "building tree 190 of 2000\n",
      "building tree 191 of 2000\n",
      "building tree 192 of 2000\n",
      "building tree 193 of 2000\n",
      "building tree 194 of 2000\n",
      "building tree 195 of 2000\n",
      "building tree 196 of 2000\n",
      "building tree 197 of 2000\n",
      "building tree 198 of 2000\n",
      "building tree 199 of 2000\n",
      "building tree 200 of 2000\n",
      "building tree 201 of 2000\n",
      "building tree 202 of 2000\n",
      "building tree 203 of 2000\n",
      "building tree 204 of 2000\n",
      "building tree 205 of 2000\n",
      "building tree 206 of 2000\n",
      "building tree 207 of 2000\n",
      "building tree 208 of 2000\n",
      "building tree 209 of 2000\n",
      "building tree 210 of 2000\n",
      "building tree 211 of 2000\n",
      "building tree 212 of 2000\n",
      "building tree 213 of 2000\n",
      "building tree 214 of 2000\n",
      "building tree 215 of 2000\n",
      "building tree 216 of 2000\n",
      "building tree 217 of 2000\n",
      "building tree 218 of 2000\n",
      "building tree 219 of 2000\n",
      "building tree 220 of 2000\n",
      "building tree 221 of 2000\n",
      "building tree 222 of 2000\n",
      "building tree 223 of 2000\n",
      "building tree 224 of 2000\n",
      "building tree 225 of 2000\n",
      "building tree 226 of 2000\n",
      "building tree 227 of 2000\n",
      "building tree 228 of 2000\n",
      "building tree 229 of 2000\n",
      "building tree 230 of 2000\n",
      "building tree 231 of 2000\n",
      "building tree 232 of 2000\n",
      "building tree 233 of 2000\n",
      "building tree 234 of 2000\n",
      "building tree 235 of 2000\n",
      "building tree 236 of 2000\n",
      "building tree 237 of 2000\n",
      "building tree 238 of 2000\n",
      "building tree 239 of 2000\n",
      "building tree 240 of 2000\n",
      "building tree 241 of 2000\n",
      "building tree 242 of 2000\n",
      "building tree 243 of 2000\n",
      "building tree 244 of 2000\n",
      "building tree 245 of 2000\n",
      "building tree 246 of 2000\n",
      "building tree 247 of 2000\n",
      "building tree 248 of 2000\n",
      "building tree 249 of 2000\n",
      "building tree 250 of 2000\n",
      "building tree 251 of 2000\n",
      "building tree 252 of 2000\n",
      "building tree 253 of 2000\n",
      "building tree 254 of 2000\n",
      "building tree 255 of 2000\n",
      "building tree 256 of 2000\n",
      "building tree 257 of 2000\n",
      "building tree 258 of 2000\n",
      "building tree 259 of 2000\n",
      "building tree 260 of 2000\n",
      "building tree 261 of 2000\n",
      "building tree 262 of 2000\n",
      "building tree 263 of 2000\n",
      "building tree 264 of 2000\n",
      "building tree 265 of 2000\n",
      "building tree 266 of 2000\n",
      "building tree 267 of 2000\n",
      "building tree 268 of 2000\n",
      "building tree 269 of 2000\n",
      "building tree 270 of 2000\n",
      "building tree 271 of 2000\n",
      "building tree 272 of 2000\n",
      "building tree 273 of 2000\n",
      "building tree 274 of 2000\n",
      "building tree 275 of 2000\n",
      "building tree 276 of 2000\n",
      "building tree 277 of 2000\n",
      "building tree 278 of 2000\n",
      "building tree 279 of 2000\n",
      "building tree 280 of 2000\n",
      "building tree 281 of 2000\n",
      "building tree 282 of 2000\n",
      "building tree 283 of 2000\n",
      "building tree 284 of 2000\n",
      "building tree 285 of 2000\n",
      "building tree 286 of 2000\n",
      "building tree 287 of 2000\n",
      "building tree 288 of 2000\n",
      "building tree 289 of 2000\n",
      "building tree 290 of 2000\n",
      "building tree 291 of 2000\n",
      "building tree 292 of 2000\n",
      "building tree 293 of 2000\n",
      "building tree 294 of 2000\n",
      "building tree 295 of 2000\n",
      "building tree 296 of 2000\n",
      "building tree 297 of 2000\n",
      "building tree 298 of 2000\n",
      "building tree 299 of 2000\n",
      "building tree 300 of 2000\n",
      "building tree 301 of 2000\n",
      "building tree 302 of 2000\n",
      "building tree 303 of 2000\n",
      "building tree 304 of 2000\n",
      "building tree 305 of 2000\n",
      "building tree 306 of 2000\n",
      "building tree 307 of 2000\n",
      "building tree 308 of 2000\n",
      "building tree 309 of 2000\n",
      "building tree 310 of 2000\n",
      "building tree 311 of 2000\n",
      "building tree 312 of 2000\n",
      "building tree 313 of 2000\n",
      "building tree 314 of 2000\n",
      "building tree 315 of 2000\n",
      "building tree 316 of 2000\n",
      "building tree 317 of 2000\n",
      "building tree 318 of 2000\n",
      "building tree 319 of 2000\n",
      "building tree 320 of 2000\n",
      "building tree 321 of 2000\n",
      "building tree 322 of 2000\n",
      "building tree 323 of 2000\n",
      "building tree 324 of 2000\n",
      "building tree 325 of 2000\n",
      "building tree 326 of 2000\n",
      "building tree 327 of 2000\n",
      "building tree 328 of 2000\n",
      "building tree 329 of 2000\n",
      "building tree 330 of 2000\n",
      "building tree 331 of 2000\n",
      "building tree 332 of 2000\n",
      "building tree 333 of 2000\n",
      "building tree 334 of 2000\n",
      "building tree 335 of 2000\n",
      "building tree 336 of 2000\n",
      "building tree 337 of 2000\n",
      "building tree 338 of 2000\n",
      "building tree 339 of 2000\n",
      "building tree 340 of 2000\n",
      "building tree 341 of 2000\n",
      "building tree 342 of 2000\n",
      "building tree 343 of 2000\n",
      "building tree 344 of 2000\n",
      "building tree 345 of 2000\n",
      "building tree 346 of 2000\n",
      "building tree 347 of 2000\n",
      "building tree 348 of 2000\n",
      "building tree 349 of 2000\n",
      "building tree 350 of 2000\n",
      "building tree 351 of 2000\n",
      "building tree 352 of 2000\n",
      "building tree 353 of 2000\n",
      "building tree 354 of 2000\n",
      "building tree 355 of 2000\n",
      "building tree 356 of 2000\n",
      "building tree 357 of 2000\n",
      "building tree 358 of 2000\n",
      "building tree 359 of 2000\n",
      "building tree 360 of 2000\n",
      "building tree 361 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  9.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 362 of 2000\n",
      "building tree 363 of 2000\n",
      "building tree 364 of 2000\n",
      "building tree 365 of 2000\n",
      "building tree 366 of 2000\n",
      "building tree 367 of 2000\n",
      "building tree 368 of 2000\n",
      "building tree 369 of 2000\n",
      "building tree 370 of 2000\n",
      "building tree 371 of 2000\n",
      "building tree 372 of 2000\n",
      "building tree 373 of 2000\n",
      "building tree 374 of 2000\n",
      "building tree 375 of 2000\n",
      "building tree 376 of 2000\n",
      "building tree 377 of 2000\n",
      "building tree 378 of 2000\n",
      "building tree 379 of 2000\n",
      "building tree 380 of 2000\n",
      "building tree 381 of 2000\n",
      "building tree 382 of 2000\n",
      "building tree 383 of 2000\n",
      "building tree 384 of 2000\n",
      "building tree 385 of 2000\n",
      "building tree 386 of 2000\n",
      "building tree 387 of 2000\n",
      "building tree 388 of 2000\n",
      "building tree 389 of 2000\n",
      "building tree 390 of 2000\n",
      "building tree 391 of 2000\n",
      "building tree 392 of 2000\n",
      "building tree 393 of 2000\n",
      "building tree 394 of 2000\n",
      "building tree 395 of 2000\n",
      "building tree 396 of 2000\n",
      "building tree 397 of 2000\n",
      "building tree 398 of 2000\n",
      "building tree 399 of 2000\n",
      "building tree 400 of 2000\n",
      "building tree 401 of 2000\n",
      "building tree 402 of 2000\n",
      "building tree 403 of 2000\n",
      "building tree 404 of 2000\n",
      "building tree 405 of 2000\n",
      "building tree 406 of 2000\n",
      "building tree 407 of 2000\n",
      "building tree 408 of 2000\n",
      "building tree 409 of 2000\n",
      "building tree 410 of 2000\n",
      "building tree 411 of 2000\n",
      "building tree 412 of 2000\n",
      "building tree 413 of 2000\n",
      "building tree 414 of 2000\n",
      "building tree 415 of 2000\n",
      "building tree 416 of 2000\n",
      "building tree 417 of 2000\n",
      "building tree 418 of 2000\n",
      "building tree 419 of 2000\n",
      "building tree 420 of 2000\n",
      "building tree 421 of 2000\n",
      "building tree 422 of 2000\n",
      "building tree 423 of 2000\n",
      "building tree 424 of 2000\n",
      "building tree 425 of 2000\n",
      "building tree 426 of 2000\n",
      "building tree 427 of 2000\n",
      "building tree 428 of 2000\n",
      "building tree 429 of 2000\n",
      "building tree 430 of 2000\n",
      "building tree 431 of 2000\n",
      "building tree 432 of 2000\n",
      "building tree 433 of 2000\n",
      "building tree 434 of 2000\n",
      "building tree 435 of 2000\n",
      "building tree 436 of 2000\n",
      "building tree 437 of 2000\n",
      "building tree 438 of 2000\n",
      "building tree 439 of 2000\n",
      "building tree 440 of 2000\n",
      "building tree 441 of 2000\n",
      "building tree 442 of 2000\n",
      "building tree 443 of 2000\n",
      "building tree 444 of 2000\n",
      "building tree 445 of 2000\n",
      "building tree 446 of 2000\n",
      "building tree 447 of 2000\n",
      "building tree 448 of 2000\n",
      "building tree 449 of 2000\n",
      "building tree 450 of 2000\n",
      "building tree 451 of 2000\n",
      "building tree 452 of 2000\n",
      "building tree 453 of 2000\n",
      "building tree 454 of 2000\n",
      "building tree 455 of 2000\n",
      "building tree 456 of 2000\n",
      "building tree 457 of 2000\n",
      "building tree 458 of 2000\n",
      "building tree 459 of 2000\n",
      "building tree 460 of 2000\n",
      "building tree 461 of 2000\n",
      "building tree 462 of 2000\n",
      "building tree 463 of 2000\n",
      "building tree 464 of 2000\n",
      "building tree 465 of 2000\n",
      "building tree 466 of 2000\n",
      "building tree 467 of 2000\n",
      "building tree 468 of 2000\n",
      "building tree 469 of 2000\n",
      "building tree 470 of 2000\n",
      "building tree 471 of 2000\n",
      "building tree 472 of 2000\n",
      "building tree 473 of 2000\n",
      "building tree 474 of 2000\n",
      "building tree 475 of 2000\n",
      "building tree 476 of 2000\n",
      "building tree 477 of 2000\n",
      "building tree 478 of 2000\n",
      "building tree 479 of 2000\n",
      "building tree 480 of 2000\n",
      "building tree 481 of 2000\n",
      "building tree 482 of 2000\n",
      "building tree 483 of 2000\n",
      "building tree 484 of 2000\n",
      "building tree 485 of 2000\n",
      "building tree 486 of 2000\n",
      "building tree 487 of 2000\n",
      "building tree 488 of 2000\n",
      "building tree 489 of 2000\n",
      "building tree 490 of 2000\n",
      "building tree 491 of 2000\n",
      "building tree 492 of 2000\n",
      "building tree 493 of 2000\n",
      "building tree 494 of 2000\n",
      "building tree 495 of 2000\n",
      "building tree 496 of 2000\n",
      "building tree 497 of 2000\n",
      "building tree 498 of 2000\n",
      "building tree 499 of 2000\n",
      "building tree 500 of 2000\n",
      "building tree 501 of 2000\n",
      "building tree 502 of 2000\n",
      "building tree 503 of 2000\n",
      "building tree 504 of 2000\n",
      "building tree 505 of 2000\n",
      "building tree 506 of 2000\n",
      "building tree 507 of 2000\n",
      "building tree 508 of 2000\n",
      "building tree 509 of 2000\n",
      "building tree 510 of 2000\n",
      "building tree 511 of 2000\n",
      "building tree 512 of 2000\n",
      "building tree 513 of 2000\n",
      "building tree 514 of 2000\n",
      "building tree 515 of 2000\n",
      "building tree 516 of 2000\n",
      "building tree 517 of 2000\n",
      "building tree 518 of 2000\n",
      "building tree 519 of 2000\n",
      "building tree 520 of 2000\n",
      "building tree 521 of 2000\n",
      "building tree 522 of 2000\n",
      "building tree 523 of 2000\n",
      "building tree 524 of 2000\n",
      "building tree 525 of 2000\n",
      "building tree 526 of 2000\n",
      "building tree 527 of 2000\n",
      "building tree 528 of 2000\n",
      "building tree 529 of 2000\n",
      "building tree 530 of 2000\n",
      "building tree 531 of 2000\n",
      "building tree 532 of 2000\n",
      "building tree 533 of 2000\n",
      "building tree 534 of 2000\n",
      "building tree 535 of 2000\n",
      "building tree 536 of 2000\n",
      "building tree 537 of 2000\n",
      "building tree 538 of 2000\n",
      "building tree 539 of 2000\n",
      "building tree 540 of 2000\n",
      "building tree 541 of 2000\n",
      "building tree 542 of 2000\n",
      "building tree 543 of 2000\n",
      "building tree 544 of 2000\n",
      "building tree 545 of 2000\n",
      "building tree 546 of 2000\n",
      "building tree 547 of 2000\n",
      "building tree 548 of 2000\n",
      "building tree 549 of 2000\n",
      "building tree 550 of 2000\n",
      "building tree 551 of 2000\n",
      "building tree 552 of 2000\n",
      "building tree 553 of 2000\n",
      "building tree 554 of 2000\n",
      "building tree 555 of 2000\n",
      "building tree 556 of 2000\n",
      "building tree 557 of 2000\n",
      "building tree 558 of 2000\n",
      "building tree 559 of 2000\n",
      "building tree 560 of 2000\n",
      "building tree 561 of 2000\n",
      "building tree 562 of 2000\n",
      "building tree 563 of 2000\n",
      "building tree 564 of 2000\n",
      "building tree 565 of 2000\n",
      "building tree 566 of 2000\n",
      "building tree 567 of 2000\n",
      "building tree 568 of 2000\n",
      "building tree 569 of 2000\n",
      "building tree 570 of 2000\n",
      "building tree 571 of 2000\n",
      "building tree 572 of 2000\n",
      "building tree 573 of 2000\n",
      "building tree 574 of 2000\n",
      "building tree 575 of 2000\n",
      "building tree 576 of 2000\n",
      "building tree 577 of 2000\n",
      "building tree 578 of 2000\n",
      "building tree 579 of 2000\n",
      "building tree 580 of 2000\n",
      "building tree 581 of 2000\n",
      "building tree 582 of 2000\n",
      "building tree 583 of 2000\n",
      "building tree 584 of 2000\n",
      "building tree 585 of 2000\n",
      "building tree 586 of 2000\n",
      "building tree 587 of 2000\n",
      "building tree 588 of 2000\n",
      "building tree 589 of 2000\n",
      "building tree 590 of 2000\n",
      "building tree 591 of 2000\n",
      "building tree 592 of 2000\n",
      "building tree 593 of 2000\n",
      "building tree 594 of 2000\n",
      "building tree 595 of 2000\n",
      "building tree 596 of 2000\n",
      "building tree 597 of 2000\n",
      "building tree 598 of 2000\n",
      "building tree 599 of 2000\n",
      "building tree 600 of 2000\n",
      "building tree 601 of 2000\n",
      "building tree 602 of 2000\n",
      "building tree 603 of 2000\n",
      "building tree 604 of 2000\n",
      "building tree 605 of 2000\n",
      "building tree 606 of 2000\n",
      "building tree 607 of 2000\n",
      "building tree 608 of 2000\n",
      "building tree 609 of 2000\n",
      "building tree 610 of 2000\n",
      "building tree 611 of 2000\n",
      "building tree 612 of 2000\n",
      "building tree 613 of 2000\n",
      "building tree 614 of 2000\n",
      "building tree 615 of 2000\n",
      "building tree 616 of 2000\n",
      "building tree 617 of 2000\n",
      "building tree 618 of 2000\n",
      "building tree 619 of 2000\n",
      "building tree 620 of 2000\n",
      "building tree 621 of 2000\n",
      "building tree 622 of 2000\n",
      "building tree 623 of 2000\n",
      "building tree 624 of 2000\n",
      "building tree 625 of 2000\n",
      "building tree 626 of 2000\n",
      "building tree 627 of 2000\n",
      "building tree 628 of 2000\n",
      "building tree 629 of 2000\n",
      "building tree 630 of 2000\n",
      "building tree 631 of 2000\n",
      "building tree 632 of 2000\n",
      "building tree 633 of 2000\n",
      "building tree 634 of 2000\n",
      "building tree 635 of 2000\n",
      "building tree 636 of 2000\n",
      "building tree 637 of 2000\n",
      "building tree 638 of 2000\n",
      "building tree 639 of 2000\n",
      "building tree 640 of 2000\n",
      "building tree 641 of 2000\n",
      "building tree 642 of 2000\n",
      "building tree 643 of 2000\n",
      "building tree 644 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 15.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 645 of 2000\n",
      "building tree 646 of 2000\n",
      "building tree 647 of 2000\n",
      "building tree 648 of 2000\n",
      "building tree 649 of 2000\n",
      "building tree 650 of 2000\n",
      "building tree 651 of 2000\n",
      "building tree 652 of 2000\n",
      "building tree 653 of 2000\n",
      "building tree 654 of 2000\n",
      "building tree 655 of 2000\n",
      "building tree 656 of 2000\n",
      "building tree 657 of 2000\n",
      "building tree 658 of 2000\n",
      "building tree 659 of 2000\n",
      "building tree 660 of 2000\n",
      "building tree 661 of 2000\n",
      "building tree 662 of 2000\n",
      "building tree 663 of 2000\n",
      "building tree 664 of 2000\n",
      "building tree 665 of 2000\n",
      "building tree 666 of 2000\n",
      "building tree 667 of 2000\n",
      "building tree 668 of 2000\n",
      "building tree 669 of 2000\n",
      "building tree 670 of 2000\n",
      "building tree 671 of 2000\n",
      "building tree 672 of 2000\n",
      "building tree 673 of 2000\n",
      "building tree 674 of 2000\n",
      "building tree 675 of 2000\n",
      "building tree 676 of 2000\n",
      "building tree 677 of 2000\n",
      "building tree 678 of 2000\n",
      "building tree 679 of 2000\n",
      "building tree 680 of 2000\n",
      "building tree 681 of 2000\n",
      "building tree 682 of 2000\n",
      "building tree 683 of 2000\n",
      "building tree 684 of 2000\n",
      "building tree 685 of 2000\n",
      "building tree 686 of 2000\n",
      "building tree 687 of 2000\n",
      "building tree 688 of 2000\n",
      "building tree 689 of 2000\n",
      "building tree 690 of 2000\n",
      "building tree 691 of 2000\n",
      "building tree 692 of 2000\n",
      "building tree 693 of 2000\n",
      "building tree 694 of 2000\n",
      "building tree 695 of 2000\n",
      "building tree 696 of 2000\n",
      "building tree 697 of 2000\n",
      "building tree 698 of 2000\n",
      "building tree 699 of 2000\n",
      "building tree 700 of 2000\n",
      "building tree 701 of 2000\n",
      "building tree 702 of 2000\n",
      "building tree 703 of 2000\n",
      "building tree 704 of 2000\n",
      "building tree 705 of 2000\n",
      "building tree 706 of 2000\n",
      "building tree 707 of 2000\n",
      "building tree 708 of 2000\n",
      "building tree 709 of 2000\n",
      "building tree 710 of 2000\n",
      "building tree 711 of 2000\n",
      "building tree 712 of 2000\n",
      "building tree 713 of 2000\n",
      "building tree 714 of 2000\n",
      "building tree 715 of 2000\n",
      "building tree 716 of 2000\n",
      "building tree 717 of 2000\n",
      "building tree 718 of 2000\n",
      "building tree 719 of 2000\n",
      "building tree 720 of 2000\n",
      "building tree 721 of 2000\n",
      "building tree 722 of 2000\n",
      "building tree 723 of 2000\n",
      "building tree 724 of 2000\n",
      "building tree 725 of 2000\n",
      "building tree 726 of 2000\n",
      "building tree 727 of 2000\n",
      "building tree 728 of 2000\n",
      "building tree 729 of 2000\n",
      "building tree 730 of 2000\n",
      "building tree 731 of 2000\n",
      "building tree 732 of 2000\n",
      "building tree 733 of 2000\n",
      "building tree 734 of 2000\n",
      "building tree 735 of 2000\n",
      "building tree 736 of 2000\n",
      "building tree 737 of 2000\n",
      "building tree 738 of 2000\n",
      "building tree 739 of 2000\n",
      "building tree 740 of 2000\n",
      "building tree 741 of 2000\n",
      "building tree 742 of 2000\n",
      "building tree 743 of 2000\n",
      "building tree 744 of 2000\n",
      "building tree 745 of 2000\n",
      "building tree 746 of 2000\n",
      "building tree 747 of 2000\n",
      "building tree 748 of 2000\n",
      "building tree 749 of 2000\n",
      "building tree 750 of 2000\n",
      "building tree 751 of 2000\n",
      "building tree 752 of 2000\n",
      "building tree 753 of 2000\n",
      "building tree 754 of 2000\n",
      "building tree 755 of 2000\n",
      "building tree 756 of 2000\n",
      "building tree 757 of 2000\n",
      "building tree 758 of 2000\n",
      "building tree 759 of 2000\n",
      "building tree 760 of 2000\n",
      "building tree 761 of 2000\n",
      "building tree 762 of 2000\n",
      "building tree 763 of 2000\n",
      "building tree 764 of 2000\n",
      "building tree 765 of 2000\n",
      "building tree 766 of 2000\n",
      "building tree 767 of 2000\n",
      "building tree 768 of 2000\n",
      "building tree 769 of 2000\n",
      "building tree 770 of 2000\n",
      "building tree 771 of 2000\n",
      "building tree 772 of 2000\n",
      "building tree 773 of 2000\n",
      "building tree 774 of 2000\n",
      "building tree 775 of 2000\n",
      "building tree 776 of 2000\n",
      "building tree 777 of 2000\n",
      "building tree 778 of 2000\n",
      "building tree 779 of 2000\n",
      "building tree 780 of 2000\n",
      "building tree 781 of 2000\n",
      "building tree 782 of 2000\n",
      "building tree 783 of 2000\n",
      "building tree 784 of 2000\n",
      "building tree 785 of 2000\n",
      "building tree 786 of 2000\n",
      "building tree 787 of 2000\n",
      "building tree 788 of 2000\n",
      "building tree 789 of 2000\n",
      "building tree 790 of 2000\n",
      "building tree 791 of 2000\n",
      "building tree 792 of 2000\n",
      "building tree 793 of 2000\n",
      "building tree 794 of 2000\n",
      "building tree 795 of 2000\n",
      "building tree 796 of 2000\n",
      "building tree 797 of 2000\n",
      "building tree 798 of 2000\n",
      "building tree 799 of 2000\n",
      "building tree 800 of 2000\n",
      "building tree 801 of 2000\n",
      "building tree 802 of 2000\n",
      "building tree 803 of 2000\n",
      "building tree 804 of 2000\n",
      "building tree 805 of 2000\n",
      "building tree 806 of 2000\n",
      "building tree 807 of 2000\n",
      "building tree 808 of 2000\n",
      "building tree 809 of 2000\n",
      "building tree 810 of 2000\n",
      "building tree 811 of 2000\n",
      "building tree 812 of 2000\n",
      "building tree 813 of 2000\n",
      "building tree 814 of 2000\n",
      "building tree 815 of 2000\n",
      "building tree 816 of 2000\n",
      "building tree 817 of 2000\n",
      "building tree 818 of 2000\n",
      "building tree 819 of 2000\n",
      "building tree 820 of 2000\n",
      "building tree 821 of 2000\n",
      "building tree 822 of 2000\n",
      "building tree 823 of 2000\n",
      "building tree 824 of 2000\n",
      "building tree 825 of 2000\n",
      "building tree 826 of 2000\n",
      "building tree 827 of 2000\n",
      "building tree 828 of 2000\n",
      "building tree 829 of 2000\n",
      "building tree 830 of 2000\n",
      "building tree 831 of 2000\n",
      "building tree 832 of 2000\n",
      "building tree 833 of 2000\n",
      "building tree 834 of 2000\n",
      "building tree 835 of 2000\n",
      "building tree 836 of 2000\n",
      "building tree 837 of 2000\n",
      "building tree 838 of 2000\n",
      "building tree 839 of 2000\n",
      "building tree 840 of 2000\n",
      "building tree 841 of 2000\n",
      "building tree 842 of 2000\n",
      "building tree 843 of 2000\n",
      "building tree 844 of 2000\n",
      "building tree 845 of 2000\n",
      "building tree 846 of 2000\n",
      "building tree 847 of 2000\n",
      "building tree 848 of 2000\n",
      "building tree 849 of 2000\n",
      "building tree 850 of 2000\n",
      "building tree 851 of 2000\n",
      "building tree 852 of 2000\n",
      "building tree 853 of 2000\n",
      "building tree 854 of 2000\n",
      "building tree 855 of 2000\n",
      "building tree 856 of 2000\n",
      "building tree 857 of 2000\n",
      "building tree 858 of 2000\n",
      "building tree 859 of 2000\n",
      "building tree 860 of 2000\n",
      "building tree 861 of 2000\n",
      "building tree 862 of 2000\n",
      "building tree 863 of 2000\n",
      "building tree 864 of 2000\n",
      "building tree 865 of 2000\n",
      "building tree 866 of 2000\n",
      "building tree 867 of 2000\n",
      "building tree 868 of 2000\n",
      "building tree 869 of 2000\n",
      "building tree 870 of 2000\n",
      "building tree 871 of 2000\n",
      "building tree 872 of 2000\n",
      "building tree 873 of 2000\n",
      "building tree 874 of 2000\n",
      "building tree 875 of 2000\n",
      "building tree 876 of 2000\n",
      "building tree 877 of 2000\n",
      "building tree 878 of 2000\n",
      "building tree 879 of 2000\n",
      "building tree 880 of 2000\n",
      "building tree 881 of 2000\n",
      "building tree 882 of 2000\n",
      "building tree 883 of 2000\n",
      "building tree 884 of 2000\n",
      "building tree 885 of 2000\n",
      "building tree 886 of 2000\n",
      "building tree 887 of 2000\n",
      "building tree 888 of 2000\n",
      "building tree 889 of 2000\n",
      "building tree 890 of 2000\n",
      "building tree 891 of 2000\n",
      "building tree 892 of 2000\n",
      "building tree 893 of 2000\n",
      "building tree 894 of 2000\n",
      "building tree 895 of 2000\n",
      "building tree 896 of 2000\n",
      "building tree 897 of 2000\n",
      "building tree 898 of 2000\n",
      "building tree 899 of 2000\n",
      "building tree 900 of 2000\n",
      "building tree 901 of 2000\n",
      "building tree 902 of 2000\n",
      "building tree 903 of 2000\n",
      "building tree 904 of 2000\n",
      "building tree 905 of 2000\n",
      "building tree 906 of 2000\n",
      "building tree 907 of 2000\n",
      "building tree 908 of 2000\n",
      "building tree 909 of 2000\n",
      "building tree 910 of 2000\n",
      "building tree 911 of 2000\n",
      "building tree 912 of 2000\n",
      "building tree 913 of 2000\n",
      "building tree 914 of 2000\n",
      "building tree 915 of 2000\n",
      "building tree 916 of 2000\n",
      "building tree 917 of 2000\n",
      "building tree 918 of 2000\n",
      "building tree 919 of 2000\n",
      "building tree 920 of 2000\n",
      "building tree 921 of 2000\n",
      "building tree 922 of 2000\n",
      "building tree 923 of 2000\n",
      "building tree 924 of 2000\n",
      "building tree 925 of 2000\n",
      "building tree 926 of 2000\n",
      "building tree 927 of 2000\n",
      "building tree 928 of 2000\n",
      "building tree 929 of 2000\n",
      "building tree 930 of 2000\n",
      "building tree 931 of 2000\n",
      "building tree 932 of 2000\n",
      "building tree 933 of 2000\n",
      "building tree 934 of 2000\n",
      "building tree 935 of 2000\n",
      "building tree 936 of 2000\n",
      "building tree 937 of 2000\n",
      "building tree 938 of 2000\n",
      "building tree 939 of 2000\n",
      "building tree 940 of 2000\n",
      "building tree 941 of 2000\n",
      "building tree 942 of 2000\n",
      "building tree 943 of 2000\n",
      "building tree 944 of 2000\n",
      "building tree 945 of 2000\n",
      "building tree 946 of 2000\n",
      "building tree 947 of 2000\n",
      "building tree 948 of 2000\n",
      "building tree 949 of 2000\n",
      "building tree 950 of 2000\n",
      "building tree 951 of 2000\n",
      "building tree 952 of 2000\n",
      "building tree 953 of 2000\n",
      "building tree 954 of 2000\n",
      "building tree 955 of 2000\n",
      "building tree 956 of 2000\n",
      "building tree 957 of 2000\n",
      "building tree 958 of 2000\n",
      "building tree 959 of 2000\n",
      "building tree 960 of 2000\n",
      "building tree 961 of 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 962 of 2000\n",
      "building tree 963 of 2000\n",
      "building tree 964 of 2000\n",
      "building tree 965 of 2000\n",
      "building tree 966 of 2000\n",
      "building tree 967 of 2000\n",
      "building tree 968 of 2000\n",
      "building tree 969 of 2000\n",
      "building tree 970 of 2000\n",
      "building tree 971 of 2000\n",
      "building tree 972 of 2000\n",
      "building tree 973 of 2000\n",
      "building tree 974 of 2000\n",
      "building tree 975 of 2000\n",
      "building tree 976 of 2000\n",
      "building tree 977 of 2000\n",
      "building tree 978 of 2000\n",
      "building tree 979 of 2000\n",
      "building tree 980 of 2000\n",
      "building tree 981 of 2000\n",
      "building tree 982 of 2000\n",
      "building tree 983 of 2000\n",
      "building tree 984 of 2000\n",
      "building tree 985 of 2000\n",
      "building tree 986 of 2000\n",
      "building tree 987 of 2000\n",
      "building tree 988 of 2000\n",
      "building tree 989 of 2000\n",
      "building tree 990 of 2000\n",
      "building tree 991 of 2000\n",
      "building tree 992 of 2000\n",
      "building tree 993 of 2000\n",
      "building tree 994 of 2000\n",
      "building tree 995 of 2000\n",
      "building tree 996 of 2000\n",
      "building tree 997 of 2000\n",
      "building tree 998 of 2000\n",
      "building tree 999 of 2000\n",
      "building tree 1000 of 2000\n",
      "building tree 1001 of 2000\n",
      "building tree 1002 of 2000\n",
      "building tree 1003 of 2000\n",
      "building tree 1004 of 2000\n",
      "building tree 1005 of 2000\n",
      "building tree 1006 of 2000\n",
      "building tree 1007 of 2000\n",
      "building tree 1008 of 2000\n",
      "building tree 1009 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 23.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1010 of 2000\n",
      "building tree 1011 of 2000\n",
      "building tree 1012 of 2000\n",
      "building tree 1013 of 2000\n",
      "building tree 1014 of 2000\n",
      "building tree 1015 of 2000\n",
      "building tree 1016 of 2000\n",
      "building tree 1017 of 2000\n",
      "building tree 1018 of 2000\n",
      "building tree 1019 of 2000\n",
      "building tree 1020 of 2000\n",
      "building tree 1021 of 2000\n",
      "building tree 1022 of 2000\n",
      "building tree 1023 of 2000\n",
      "building tree 1024 of 2000\n",
      "building tree 1025 of 2000\n",
      "building tree 1026 of 2000\n",
      "building tree 1027 of 2000\n",
      "building tree 1028 of 2000\n",
      "building tree 1029 of 2000\n",
      "building tree 1030 of 2000\n",
      "building tree 1031 of 2000\n",
      "building tree 1032 of 2000\n",
      "building tree 1033 of 2000\n",
      "building tree 1034 of 2000\n",
      "building tree 1035 of 2000\n",
      "building tree 1036 of 2000\n",
      "building tree 1037 of 2000\n",
      "building tree 1038 of 2000\n",
      "building tree 1039 of 2000\n",
      "building tree 1040 of 2000\n",
      "building tree 1041 of 2000\n",
      "building tree 1042 of 2000\n",
      "building tree 1043 of 2000\n",
      "building tree 1044 of 2000\n",
      "building tree 1045 of 2000\n",
      "building tree 1046 of 2000\n",
      "building tree 1047 of 2000\n",
      "building tree 1048 of 2000\n",
      "building tree 1049 of 2000\n",
      "building tree 1050 of 2000\n",
      "building tree 1051 of 2000\n",
      "building tree 1052 of 2000\n",
      "building tree 1053 of 2000\n",
      "building tree 1054 of 2000\n",
      "building tree 1055 of 2000\n",
      "building tree 1056 of 2000\n",
      "building tree 1057 of 2000\n",
      "building tree 1058 of 2000\n",
      "building tree 1059 of 2000\n",
      "building tree 1060 of 2000\n",
      "building tree 1061 of 2000\n",
      "building tree 1062 of 2000\n",
      "building tree 1063 of 2000\n",
      "building tree 1064 of 2000\n",
      "building tree 1065 of 2000\n",
      "building tree 1066 of 2000\n",
      "building tree 1067 of 2000\n",
      "building tree 1068 of 2000\n",
      "building tree 1069 of 2000\n",
      "building tree 1070 of 2000\n",
      "building tree 1071 of 2000\n",
      "building tree 1072 of 2000\n",
      "building tree 1073 of 2000\n",
      "building tree 1074 of 2000\n",
      "building tree 1075 of 2000\n",
      "building tree 1076 of 2000\n",
      "building tree 1077 of 2000\n",
      "building tree 1078 of 2000\n",
      "building tree 1079 of 2000\n",
      "building tree 1080 of 2000\n",
      "building tree 1081 of 2000\n",
      "building tree 1082 of 2000\n",
      "building tree 1083 of 2000\n",
      "building tree 1084 of 2000\n",
      "building tree 1085 of 2000\n",
      "building tree 1086 of 2000\n",
      "building tree 1087 of 2000\n",
      "building tree 1088 of 2000\n",
      "building tree 1089 of 2000\n",
      "building tree 1090 of 2000\n",
      "building tree 1091 of 2000\n",
      "building tree 1092 of 2000\n",
      "building tree 1093 of 2000\n",
      "building tree 1094 of 2000\n",
      "building tree 1095 of 2000\n",
      "building tree 1096 of 2000\n",
      "building tree 1097 of 2000\n",
      "building tree 1098 of 2000\n",
      "building tree 1099 of 2000\n",
      "building tree 1100 of 2000\n",
      "building tree 1101 of 2000\n",
      "building tree 1102 of 2000\n",
      "building tree 1103 of 2000\n",
      "building tree 1104 of 2000\n",
      "building tree 1105 of 2000\n",
      "building tree 1106 of 2000\n",
      "building tree 1107 of 2000\n",
      "building tree 1108 of 2000\n",
      "building tree 1109 of 2000\n",
      "building tree 1110 of 2000\n",
      "building tree 1111 of 2000\n",
      "building tree 1112 of 2000\n",
      "building tree 1113 of 2000\n",
      "building tree 1114 of 2000\n",
      "building tree 1115 of 2000\n",
      "building tree 1116 of 2000\n",
      "building tree 1117 of 2000\n",
      "building tree 1118 of 2000\n",
      "building tree 1119 of 2000\n",
      "building tree 1120 of 2000\n",
      "building tree 1121 of 2000\n",
      "building tree 1122 of 2000\n",
      "building tree 1123 of 2000\n",
      "building tree 1124 of 2000\n",
      "building tree 1125 of 2000\n",
      "building tree 1126 of 2000\n",
      "building tree 1127 of 2000\n",
      "building tree 1128 of 2000\n",
      "building tree 1129 of 2000\n",
      "building tree 1130 of 2000\n",
      "building tree 1131 of 2000\n",
      "building tree 1132 of 2000\n",
      "building tree 1133 of 2000\n",
      "building tree 1134 of 2000\n",
      "building tree 1135 of 2000\n",
      "building tree 1136 of 2000\n",
      "building tree 1137 of 2000\n",
      "building tree 1138 of 2000\n",
      "building tree 1139 of 2000\n",
      "building tree 1140 of 2000\n",
      "building tree 1141 of 2000\n",
      "building tree 1142 of 2000\n",
      "building tree 1143 of 2000\n",
      "building tree 1144 of 2000\n",
      "building tree 1145 of 2000\n",
      "building tree 1146 of 2000\n",
      "building tree 1147 of 2000\n",
      "building tree 1148 of 2000\n",
      "building tree 1149 of 2000\n",
      "building tree 1150 of 2000\n",
      "building tree 1151 of 2000\n",
      "building tree 1152 of 2000\n",
      "building tree 1153 of 2000\n",
      "building tree 1154 of 2000\n",
      "building tree 1155 of 2000\n",
      "building tree 1156 of 2000\n",
      "building tree 1157 of 2000\n",
      "building tree 1158 of 2000\n",
      "building tree 1159 of 2000\n",
      "building tree 1160 of 2000\n",
      "building tree 1161 of 2000\n",
      "building tree 1162 of 2000\n",
      "building tree 1163 of 2000\n",
      "building tree 1164 of 2000\n",
      "building tree 1165 of 2000\n",
      "building tree 1166 of 2000\n",
      "building tree 1167 of 2000\n",
      "building tree 1168 of 2000\n",
      "building tree 1169 of 2000\n",
      "building tree 1170 of 2000\n",
      "building tree 1171 of 2000\n",
      "building tree 1172 of 2000\n",
      "building tree 1173 of 2000\n",
      "building tree 1174 of 2000\n",
      "building tree 1175 of 2000\n",
      "building tree 1176 of 2000\n",
      "building tree 1177 of 2000\n",
      "building tree 1178 of 2000\n",
      "building tree 1179 of 2000\n",
      "building tree 1180 of 2000\n",
      "building tree 1181 of 2000\n",
      "building tree 1182 of 2000\n",
      "building tree 1183 of 2000\n",
      "building tree 1184 of 2000\n",
      "building tree 1185 of 2000\n",
      "building tree 1186 of 2000\n",
      "building tree 1187 of 2000\n",
      "building tree 1188 of 2000\n",
      "building tree 1189 of 2000\n",
      "building tree 1190 of 2000\n",
      "building tree 1191 of 2000\n",
      "building tree 1192 of 2000\n",
      "building tree 1193 of 2000\n",
      "building tree 1194 of 2000\n",
      "building tree 1195 of 2000\n",
      "building tree 1196 of 2000\n",
      "building tree 1197 of 2000\n",
      "building tree 1198 of 2000\n",
      "building tree 1199 of 2000\n",
      "building tree 1200 of 2000\n",
      "building tree 1201 of 2000\n",
      "building tree 1202 of 2000\n",
      "building tree 1203 of 2000\n",
      "building tree 1204 of 2000\n",
      "building tree 1205 of 2000\n",
      "building tree 1206 of 2000\n",
      "building tree 1207 of 2000\n",
      "building tree 1208 of 2000\n",
      "building tree 1209 of 2000\n",
      "building tree 1210 of 2000\n",
      "building tree 1211 of 2000\n",
      "building tree 1212 of 2000\n",
      "building tree 1213 of 2000\n",
      "building tree 1214 of 2000\n",
      "building tree 1215 of 2000\n",
      "building tree 1216 of 2000\n",
      "building tree 1217 of 2000\n",
      "building tree 1218 of 2000\n",
      "building tree 1219 of 2000\n",
      "building tree 1220 of 2000\n",
      "building tree 1221 of 2000\n",
      "building tree 1222 of 2000\n",
      "building tree 1223 of 2000\n",
      "building tree 1224 of 2000\n",
      "building tree 1225 of 2000\n",
      "building tree 1226 of 2000\n",
      "building tree 1227 of 2000\n",
      "building tree 1228 of 2000\n",
      "building tree 1229 of 2000\n",
      "building tree 1230 of 2000\n",
      "building tree 1231 of 2000\n",
      "building tree 1232 of 2000\n",
      "building tree 1233 of 2000\n",
      "building tree 1234 of 2000\n",
      "building tree 1235 of 2000\n",
      "building tree 1236 of 2000\n",
      "building tree 1237 of 2000\n",
      "building tree 1238 of 2000\n",
      "building tree 1239 of 2000\n",
      "building tree 1240 of 2000\n",
      "building tree 1241 of 2000\n",
      "building tree 1242 of 2000\n",
      "building tree 1243 of 2000\n",
      "building tree 1244 of 2000\n",
      "building tree 1245 of 2000\n",
      "building tree 1246 of 2000\n",
      "building tree 1247 of 2000\n",
      "building tree 1248 of 2000\n",
      "building tree 1249 of 2000\n",
      "building tree 1250 of 2000\n",
      "building tree 1251 of 2000\n",
      "building tree 1252 of 2000\n",
      "building tree 1253 of 2000\n",
      "building tree 1254 of 2000\n",
      "building tree 1255 of 2000\n",
      "building tree 1256 of 2000\n",
      "building tree 1257 of 2000\n",
      "building tree 1258 of 2000\n",
      "building tree 1259 of 2000\n",
      "building tree 1260 of 2000\n",
      "building tree 1261 of 2000\n",
      "building tree 1262 of 2000\n",
      "building tree 1263 of 2000\n",
      "building tree 1264 of 2000\n",
      "building tree 1265 of 2000\n",
      "building tree 1266 of 2000\n",
      "building tree 1267 of 2000\n",
      "building tree 1268 of 2000\n",
      "building tree 1269 of 2000\n",
      "building tree 1270 of 2000\n",
      "building tree 1271 of 2000\n",
      "building tree 1272 of 2000\n",
      "building tree 1273 of 2000\n",
      "building tree 1274 of 2000\n",
      "building tree 1275 of 2000\n",
      "building tree 1276 of 2000\n",
      "building tree 1277 of 2000\n",
      "building tree 1278 of 2000\n",
      "building tree 1279 of 2000\n",
      "building tree 1280 of 2000\n",
      "building tree 1281 of 2000\n",
      "building tree 1282 of 2000\n",
      "building tree 1283 of 2000\n",
      "building tree 1284 of 2000\n",
      "building tree 1285 of 2000\n",
      "building tree 1286 of 2000\n",
      "building tree 1287 of 2000\n",
      "building tree 1288 of 2000\n",
      "building tree 1289 of 2000\n",
      "building tree 1290 of 2000\n",
      "building tree 1291 of 2000\n",
      "building tree 1292 of 2000\n",
      "building tree 1293 of 2000\n",
      "building tree 1294 of 2000\n",
      "building tree 1295 of 2000\n",
      "building tree 1296 of 2000\n",
      "building tree 1297 of 2000\n",
      "building tree 1298 of 2000\n",
      "building tree 1299 of 2000\n",
      "building tree 1300 of 2000\n",
      "building tree 1301 of 2000\n",
      "building tree 1302 of 2000\n",
      "building tree 1303 of 2000\n",
      "building tree 1304 of 2000\n",
      "building tree 1305 of 2000\n",
      "building tree 1306 of 2000\n",
      "building tree 1307 of 2000\n",
      "building tree 1308 of 2000\n",
      "building tree 1309 of 2000\n",
      "building tree 1310 of 2000\n",
      "building tree 1311 of 2000\n",
      "building tree 1312 of 2000\n",
      "building tree 1313 of 2000\n",
      "building tree 1314 of 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1315 of 2000\n",
      "building tree 1316 of 2000\n",
      "building tree 1317 of 2000\n",
      "building tree 1318 of 2000\n",
      "building tree 1319 of 2000\n",
      "building tree 1320 of 2000\n",
      "building tree 1321 of 2000\n",
      "building tree 1322 of 2000\n",
      "building tree 1323 of 2000\n",
      "building tree 1324 of 2000\n",
      "building tree 1325 of 2000\n",
      "building tree 1326 of 2000\n",
      "building tree 1327 of 2000\n",
      "building tree 1328 of 2000\n",
      "building tree 1329 of 2000\n",
      "building tree 1330 of 2000\n",
      "building tree 1331 of 2000\n",
      "building tree 1332 of 2000\n",
      "building tree 1333 of 2000\n",
      "building tree 1334 of 2000\n",
      "building tree 1335 of 2000\n",
      "building tree 1336 of 2000\n",
      "building tree 1337 of 2000\n",
      "building tree 1338 of 2000\n",
      "building tree 1339 of 2000\n",
      "building tree 1340 of 2000\n",
      "building tree 1341 of 2000\n",
      "building tree 1342 of 2000\n",
      "building tree 1343 of 2000\n",
      "building tree 1344 of 2000\n",
      "building tree 1345 of 2000\n",
      "building tree 1346 of 2000\n",
      "building tree 1347 of 2000\n",
      "building tree 1348 of 2000\n",
      "building tree 1349 of 2000\n",
      "building tree 1350 of 2000\n",
      "building tree 1351 of 2000\n",
      "building tree 1352 of 2000\n",
      "building tree 1353 of 2000\n",
      "building tree 1354 of 2000\n",
      "building tree 1355 of 2000\n",
      "building tree 1356 of 2000\n",
      "building tree 1357 of 2000\n",
      "building tree 1358 of 2000\n",
      "building tree 1359 of 2000\n",
      "building tree 1360 of 2000\n",
      "building tree 1361 of 2000\n",
      "building tree 1362 of 2000\n",
      "building tree 1363 of 2000\n",
      "building tree 1364 of 2000\n",
      "building tree 1365 of 2000\n",
      "building tree 1366 of 2000\n",
      "building tree 1367 of 2000\n",
      "building tree 1368 of 2000\n",
      "building tree 1369 of 2000\n",
      "building tree 1370 of 2000\n",
      "building tree 1371 of 2000\n",
      "building tree 1372 of 2000\n",
      "building tree 1373 of 2000\n",
      "building tree 1374 of 2000\n",
      "building tree 1375 of 2000\n",
      "building tree 1376 of 2000\n",
      "building tree 1377 of 2000\n",
      "building tree 1378 of 2000\n",
      "building tree 1379 of 2000\n",
      "building tree 1380 of 2000\n",
      "building tree 1381 of 2000\n",
      "building tree 1382 of 2000\n",
      "building tree 1383 of 2000\n",
      "building tree 1384 of 2000\n",
      "building tree 1385 of 2000\n",
      "building tree 1386 of 2000\n",
      "building tree 1387 of 2000\n",
      "building tree 1388 of 2000\n",
      "building tree 1389 of 2000\n",
      "building tree 1390 of 2000\n",
      "building tree 1391 of 2000\n",
      "building tree 1392 of 2000\n",
      "building tree 1393 of 2000\n",
      "building tree 1394 of 2000\n",
      "building tree 1395 of 2000\n",
      "building tree 1396 of 2000\n",
      "building tree 1397 of 2000\n",
      "building tree 1398 of 2000\n",
      "building tree 1399 of 2000\n",
      "building tree 1400 of 2000\n",
      "building tree 1401 of 2000\n",
      "building tree 1402 of 2000\n",
      "building tree 1403 of 2000\n",
      "building tree 1404 of 2000\n",
      "building tree 1405 of 2000\n",
      "building tree 1406 of 2000\n",
      "building tree 1407 of 2000\n",
      "building tree 1408 of 2000\n",
      "building tree 1409 of 2000\n",
      "building tree 1410 of 2000\n",
      "building tree 1411 of 2000\n",
      "building tree 1412 of 2000\n",
      "building tree 1413 of 2000\n",
      "building tree 1414 of 2000\n",
      "building tree 1415 of 2000\n",
      "building tree 1416 of 2000\n",
      "building tree 1417 of 2000\n",
      "building tree 1418 of 2000\n",
      "building tree 1419 of 2000\n",
      "building tree 1420 of 2000\n",
      "building tree 1421 of 2000\n",
      "building tree 1422 of 2000\n",
      "building tree 1423 of 2000\n",
      "building tree 1424 of 2000\n",
      "building tree 1425 of 2000\n",
      "building tree 1426 of 2000\n",
      "building tree 1427 of 2000\n",
      "building tree 1428 of 2000\n",
      "building tree 1429 of 2000\n",
      "building tree 1430 of 2000\n",
      "building tree 1431 of 2000\n",
      "building tree 1432 of 2000\n",
      "building tree 1433 of 2000\n",
      "building tree 1434 of 2000\n",
      "building tree 1435 of 2000\n",
      "building tree 1436 of 2000\n",
      "building tree 1437 of 2000\n",
      "building tree 1438 of 2000\n",
      "building tree 1439 of 2000\n",
      "building tree 1440 of 2000\n",
      "building tree 1441 of 2000\n",
      "building tree 1442 of 2000\n",
      "building tree 1443 of 2000\n",
      "building tree 1444 of 2000\n",
      "building tree 1445 of 2000\n",
      "building tree 1446 of 2000\n",
      "building tree 1447 of 2000\n",
      "building tree 1448 of 2000\n",
      "building tree 1449 of 2000\n",
      "building tree 1450 of 2000\n",
      "building tree 1451 of 2000\n",
      "building tree 1452 of 2000\n",
      "building tree 1453 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 33.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1454 of 2000\n",
      "building tree 1455 of 2000\n",
      "building tree 1456 of 2000\n",
      "building tree 1457 of 2000\n",
      "building tree 1458 of 2000\n",
      "building tree 1459 of 2000\n",
      "building tree 1460 of 2000\n",
      "building tree 1461 of 2000\n",
      "building tree 1462 of 2000\n",
      "building tree 1463 of 2000\n",
      "building tree 1464 of 2000\n",
      "building tree 1465 of 2000\n",
      "building tree 1466 of 2000\n",
      "building tree 1467 of 2000\n",
      "building tree 1468 of 2000\n",
      "building tree 1469 of 2000\n",
      "building tree 1470 of 2000\n",
      "building tree 1471 of 2000\n",
      "building tree 1472 of 2000\n",
      "building tree 1473 of 2000\n",
      "building tree 1474 of 2000\n",
      "building tree 1475 of 2000\n",
      "building tree 1476 of 2000\n",
      "building tree 1477 of 2000\n",
      "building tree 1478 of 2000\n",
      "building tree 1479 of 2000\n",
      "building tree 1480 of 2000\n",
      "building tree 1481 of 2000\n",
      "building tree 1482 of 2000\n",
      "building tree 1483 of 2000\n",
      "building tree 1484 of 2000\n",
      "building tree 1485 of 2000\n",
      "building tree 1486 of 2000\n",
      "building tree 1487 of 2000\n",
      "building tree 1488 of 2000\n",
      "building tree 1489 of 2000\n",
      "building tree 1490 of 2000\n",
      "building tree 1491 of 2000\n",
      "building tree 1492 of 2000\n",
      "building tree 1493 of 2000\n",
      "building tree 1494 of 2000\n",
      "building tree 1495 of 2000\n",
      "building tree 1496 of 2000\n",
      "building tree 1497 of 2000\n",
      "building tree 1498 of 2000\n",
      "building tree 1499 of 2000\n",
      "building tree 1500 of 2000\n",
      "building tree 1501 of 2000\n",
      "building tree 1502 of 2000\n",
      "building tree 1503 of 2000\n",
      "building tree 1504 of 2000\n",
      "building tree 1505 of 2000\n",
      "building tree 1506 of 2000\n",
      "building tree 1507 of 2000\n",
      "building tree 1508 of 2000\n",
      "building tree 1509 of 2000\n",
      "building tree 1510 of 2000\n",
      "building tree 1511 of 2000\n",
      "building tree 1512 of 2000\n",
      "building tree 1513 of 2000\n",
      "building tree 1514 of 2000\n",
      "building tree 1515 of 2000\n",
      "building tree 1516 of 2000\n",
      "building tree 1517 of 2000\n",
      "building tree 1518 of 2000\n",
      "building tree 1519 of 2000\n",
      "building tree 1520 of 2000\n",
      "building tree 1521 of 2000\n",
      "building tree 1522 of 2000\n",
      "building tree 1523 of 2000\n",
      "building tree 1524 of 2000\n",
      "building tree 1525 of 2000\n",
      "building tree 1526 of 2000\n",
      "building tree 1527 of 2000\n",
      "building tree 1528 of 2000\n",
      "building tree 1529 of 2000\n",
      "building tree 1530 of 2000\n",
      "building tree 1531 of 2000\n",
      "building tree 1532 of 2000\n",
      "building tree 1533 of 2000\n",
      "building tree 1534 of 2000\n",
      "building tree 1535 of 2000\n",
      "building tree 1536 of 2000\n",
      "building tree 1537 of 2000\n",
      "building tree 1538 of 2000\n",
      "building tree 1539 of 2000\n",
      "building tree 1540 of 2000\n",
      "building tree 1541 of 2000\n",
      "building tree 1542 of 2000\n",
      "building tree 1543 of 2000\n",
      "building tree 1544 of 2000\n",
      "building tree 1545 of 2000\n",
      "building tree 1546 of 2000\n",
      "building tree 1547 of 2000\n",
      "building tree 1548 of 2000\n",
      "building tree 1549 of 2000\n",
      "building tree 1550 of 2000\n",
      "building tree 1551 of 2000\n",
      "building tree 1552 of 2000\n",
      "building tree 1553 of 2000\n",
      "building tree 1554 of 2000\n",
      "building tree 1555 of 2000\n",
      "building tree 1556 of 2000\n",
      "building tree 1557 of 2000\n",
      "building tree 1558 of 2000\n",
      "building tree 1559 of 2000\n",
      "building tree 1560 of 2000\n",
      "building tree 1561 of 2000\n",
      "building tree 1562 of 2000\n",
      "building tree 1563 of 2000\n",
      "building tree 1564 of 2000\n",
      "building tree 1565 of 2000\n",
      "building tree 1566 of 2000\n",
      "building tree 1567 of 2000\n",
      "building tree 1568 of 2000\n",
      "building tree 1569 of 2000\n",
      "building tree 1570 of 2000\n",
      "building tree 1571 of 2000\n",
      "building tree 1572 of 2000\n",
      "building tree 1573 of 2000\n",
      "building tree 1574 of 2000\n",
      "building tree 1575 of 2000\n",
      "building tree 1576 of 2000\n",
      "building tree 1577 of 2000\n",
      "building tree 1578 of 2000\n",
      "building tree 1579 of 2000\n",
      "building tree 1580 of 2000\n",
      "building tree 1581 of 2000\n",
      "building tree 1582 of 2000\n",
      "building tree 1583 of 2000\n",
      "building tree 1584 of 2000\n",
      "building tree 1585 of 2000\n",
      "building tree 1586 of 2000\n",
      "building tree 1587 of 2000\n",
      "building tree 1588 of 2000\n",
      "building tree 1589 of 2000\n",
      "building tree 1590 of 2000\n",
      "building tree 1591 of 2000\n",
      "building tree 1592 of 2000\n",
      "building tree 1593 of 2000\n",
      "building tree 1594 of 2000\n",
      "building tree 1595 of 2000\n",
      "building tree 1596 of 2000\n",
      "building tree 1597 of 2000\n",
      "building tree 1598 of 2000\n",
      "building tree 1599 of 2000\n",
      "building tree 1600 of 2000\n",
      "building tree 1601 of 2000\n",
      "building tree 1602 of 2000\n",
      "building tree 1603 of 2000\n",
      "building tree 1604 of 2000\n",
      "building tree 1605 of 2000\n",
      "building tree 1606 of 2000\n",
      "building tree 1607 of 2000\n",
      "building tree 1608 of 2000\n",
      "building tree 1609 of 2000\n",
      "building tree 1610 of 2000\n",
      "building tree 1611 of 2000\n",
      "building tree 1612 of 2000\n",
      "building tree 1613 of 2000\n",
      "building tree 1614 of 2000\n",
      "building tree 1615 of 2000\n",
      "building tree 1616 of 2000\n",
      "building tree 1617 of 2000\n",
      "building tree 1618 of 2000\n",
      "building tree 1619 of 2000\n",
      "building tree 1620 of 2000\n",
      "building tree 1621 of 2000\n",
      "building tree 1622 of 2000\n",
      "building tree 1623 of 2000\n",
      "building tree 1624 of 2000\n",
      "building tree 1625 of 2000\n",
      "building tree 1626 of 2000\n",
      "building tree 1627 of 2000\n",
      "building tree 1628 of 2000\n",
      "building tree 1629 of 2000\n",
      "building tree 1630 of 2000\n",
      "building tree 1631 of 2000\n",
      "building tree 1632 of 2000\n",
      "building tree 1633 of 2000\n",
      "building tree 1634 of 2000\n",
      "building tree 1635 of 2000\n",
      "building tree 1636 of 2000\n",
      "building tree 1637 of 2000\n",
      "building tree 1638 of 2000\n",
      "building tree 1639 of 2000\n",
      "building tree 1640 of 2000\n",
      "building tree 1641 of 2000\n",
      "building tree 1642 of 2000\n",
      "building tree 1643 of 2000\n",
      "building tree 1644 of 2000\n",
      "building tree 1645 of 2000\n",
      "building tree 1646 of 2000\n",
      "building tree 1647 of 2000\n",
      "building tree 1648 of 2000\n",
      "building tree 1649 of 2000\n",
      "building tree 1650 of 2000\n",
      "building tree 1651 of 2000\n",
      "building tree 1652 of 2000\n",
      "building tree 1653 of 2000\n",
      "building tree 1654 of 2000\n",
      "building tree 1655 of 2000\n",
      "building tree 1656 of 2000\n",
      "building tree 1657 of 2000\n",
      "building tree 1658 of 2000\n",
      "building tree 1659 of 2000\n",
      "building tree 1660 of 2000\n",
      "building tree 1661 of 2000\n",
      "building tree 1662 of 2000\n",
      "building tree 1663 of 2000\n",
      "building tree 1664 of 2000\n",
      "building tree 1665 of 2000\n",
      "building tree 1666 of 2000\n",
      "building tree 1667 of 2000\n",
      "building tree 1668 of 2000\n",
      "building tree 1669 of 2000\n",
      "building tree 1670 of 2000\n",
      "building tree 1671 of 2000\n",
      "building tree 1672 of 2000\n",
      "building tree 1673 of 2000\n",
      "building tree 1674 of 2000\n",
      "building tree 1675 of 2000\n",
      "building tree 1676 of 2000\n",
      "building tree 1677 of 2000\n",
      "building tree 1678 of 2000\n",
      "building tree 1679 of 2000\n",
      "building tree 1680 of 2000\n",
      "building tree 1681 of 2000\n",
      "building tree 1682 of 2000\n",
      "building tree 1683 of 2000\n",
      "building tree 1684 of 2000\n",
      "building tree 1685 of 2000\n",
      "building tree 1686 of 2000\n",
      "building tree 1687 of 2000\n",
      "building tree 1688 of 2000\n",
      "building tree 1689 of 2000\n",
      "building tree 1690 of 2000\n",
      "building tree 1691 of 2000\n",
      "building tree 1692 of 2000\n",
      "building tree 1693 of 2000\n",
      "building tree 1694 of 2000\n",
      "building tree 1695 of 2000\n",
      "building tree 1696 of 2000\n",
      "building tree 1697 of 2000\n",
      "building tree 1698 of 2000\n",
      "building tree 1699 of 2000\n",
      "building tree 1700 of 2000\n",
      "building tree 1701 of 2000\n",
      "building tree 1702 of 2000\n",
      "building tree 1703 of 2000\n",
      "building tree 1704 of 2000\n",
      "building tree 1705 of 2000\n",
      "building tree 1706 of 2000\n",
      "building tree 1707 of 2000\n",
      "building tree 1708 of 2000\n",
      "building tree 1709 of 2000\n",
      "building tree 1710 of 2000\n",
      "building tree 1711 of 2000\n",
      "building tree 1712 of 2000\n",
      "building tree 1713 of 2000\n",
      "building tree 1714 of 2000\n",
      "building tree 1715 of 2000\n",
      "building tree 1716 of 2000\n",
      "building tree 1717 of 2000\n",
      "building tree 1718 of 2000\n",
      "building tree 1719 of 2000\n",
      "building tree 1720 of 2000\n",
      "building tree 1721 of 2000\n",
      "building tree 1722 of 2000\n",
      "building tree 1723 of 2000\n",
      "building tree 1724 of 2000\n",
      "building tree 1725 of 2000\n",
      "building tree 1726 of 2000\n",
      "building tree 1727 of 2000\n",
      "building tree 1728 of 2000\n",
      "building tree 1729 of 2000\n",
      "building tree 1730 of 2000\n",
      "building tree 1731 of 2000\n",
      "building tree 1732 of 2000\n",
      "building tree 1733 of 2000\n",
      "building tree 1734 of 2000\n",
      "building tree 1735 of 2000\n",
      "building tree 1736 of 2000\n",
      "building tree 1737 of 2000\n",
      "building tree 1738 of 2000\n",
      "building tree 1739 of 2000\n",
      "building tree 1740 of 2000\n",
      "building tree 1741 of 2000\n",
      "building tree 1742 of 2000\n",
      "building tree 1743 of 2000\n",
      "building tree 1744 of 2000\n",
      "building tree 1745 of 2000\n",
      "building tree 1746 of 2000\n",
      "building tree 1747 of 2000\n",
      "building tree 1748 of 2000\n",
      "building tree 1749 of 2000\n",
      "building tree 1750 of 2000\n",
      "building tree 1751 of 2000\n",
      "building tree 1752 of 2000\n",
      "building tree 1753 of 2000\n",
      "building tree 1754 of 2000\n",
      "building tree 1755 of 2000\n",
      "building tree 1756 of 2000\n",
      "building tree 1757 of 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1758 of 2000\n",
      "building tree 1759 of 2000\n",
      "building tree 1760 of 2000\n",
      "building tree 1761 of 2000\n",
      "building tree 1762 of 2000\n",
      "building tree 1763 of 2000\n",
      "building tree 1764 of 2000\n",
      "building tree 1765 of 2000\n",
      "building tree 1766 of 2000\n",
      "building tree 1767 of 2000\n",
      "building tree 1768 of 2000\n",
      "building tree 1769 of 2000\n",
      "building tree 1770 of 2000\n",
      "building tree 1771 of 2000\n",
      "building tree 1772 of 2000\n",
      "building tree 1773 of 2000\n",
      "building tree 1774 of 2000\n",
      "building tree 1775 of 2000\n",
      "building tree 1776 of 2000\n",
      "building tree 1777 of 2000\n",
      "building tree 1778 of 2000\n",
      "building tree 1779 of 2000\n",
      "building tree 1780 of 2000\n",
      "building tree 1781 of 2000\n",
      "building tree 1782 of 2000\n",
      "building tree 1783 of 2000\n",
      "building tree 1784 of 2000\n",
      "building tree 1785 of 2000\n",
      "building tree 1786 of 2000\n",
      "building tree 1787 of 2000\n",
      "building tree 1788 of 2000\n",
      "building tree 1789 of 2000\n",
      "building tree 1790 of 2000\n",
      "building tree 1791 of 2000\n",
      "building tree 1792 of 2000\n",
      "building tree 1793 of 2000\n",
      "building tree 1794 of 2000\n",
      "building tree 1795 of 2000\n",
      "building tree 1796 of 2000\n",
      "building tree 1797 of 2000\n",
      "building tree 1798 of 2000\n",
      "building tree 1799 of 2000\n",
      "building tree 1800 of 2000\n",
      "building tree 1801 of 2000\n",
      "building tree 1802 of 2000\n",
      "building tree 1803 of 2000\n",
      "building tree 1804 of 2000\n",
      "building tree 1805 of 2000\n",
      "building tree 1806 of 2000\n",
      "building tree 1807 of 2000\n",
      "building tree 1808 of 2000\n",
      "building tree 1809 of 2000\n",
      "building tree 1810 of 2000\n",
      "building tree 1811 of 2000building tree 1812 of 2000\n",
      "\n",
      "building tree 1813 of 2000\n",
      "building tree 1814 of 2000\n",
      "building tree 1815 of 2000\n",
      "building tree 1816 of 2000\n",
      "building tree 1817 of 2000\n",
      "building tree 1818 of 2000\n",
      "building tree 1819 of 2000\n",
      "building tree 1820 of 2000\n",
      "building tree 1821 of 2000\n",
      "building tree 1822 of 2000\n",
      "building tree 1823 of 2000\n",
      "building tree 1824 of 2000\n",
      "building tree 1825 of 2000\n",
      "building tree 1826 of 2000\n",
      "building tree 1827 of 2000\n",
      "building tree 1828 of 2000\n",
      "building tree 1829 of 2000\n",
      "building tree 1830 of 2000\n",
      "building tree 1831 of 2000\n",
      "building tree 1832 of 2000\n",
      "building tree 1833 of 2000\n",
      "building tree 1834 of 2000\n",
      "building tree 1835 of 2000\n",
      "building tree 1836 of 2000\n",
      "building tree 1837 of 2000\n",
      "building tree 1838 of 2000\n",
      "building tree 1839 of 2000\n",
      "building tree 1840 of 2000\n",
      "building tree 1841 of 2000\n",
      "building tree 1842 of 2000\n",
      "building tree 1843 of 2000\n",
      "building tree 1844 of 2000\n",
      "building tree 1845 of 2000\n",
      "building tree 1846 of 2000\n",
      "building tree 1847 of 2000\n",
      "building tree 1848 of 2000\n",
      "building tree 1849 of 2000\n",
      "building tree 1850 of 2000\n",
      "building tree 1851 of 2000\n",
      "building tree 1852 of 2000\n",
      "building tree 1853 of 2000\n",
      "building tree 1854 of 2000\n",
      "building tree 1855 of 2000\n",
      "building tree 1856 of 2000\n",
      "building tree 1857 of 2000\n",
      "building tree 1858 of 2000\n",
      "building tree 1859 of 2000\n",
      "building tree 1860 of 2000\n",
      "building tree 1861 of 2000\n",
      "building tree 1862 of 2000\n",
      "building tree 1863 of 2000\n",
      "building tree 1864 of 2000\n",
      "building tree 1865 of 2000\n",
      "building tree 1866 of 2000\n",
      "building tree 1867 of 2000\n",
      "building tree 1868 of 2000\n",
      "building tree 1869 of 2000\n",
      "building tree 1870 of 2000\n",
      "building tree 1871 of 2000\n",
      "building tree 1872 of 2000\n",
      "building tree 1873 of 2000\n",
      "building tree 1874 of 2000\n",
      "building tree 1875 of 2000\n",
      "building tree 1876 of 2000\n",
      "building tree 1877 of 2000\n",
      "building tree 1878 of 2000\n",
      "building tree 1879 of 2000\n",
      "building tree 1880 of 2000\n",
      "building tree 1881 of 2000\n",
      "building tree 1882 of 2000\n",
      "building tree 1883 of 2000\n",
      "building tree 1884 of 2000\n",
      "building tree 1885 of 2000\n",
      "building tree 1886 of 2000\n",
      "building tree 1887 of 2000\n",
      "building tree 1888 of 2000\n",
      "building tree 1889 of 2000\n",
      "building tree 1890 of 2000\n",
      "building tree 1891 of 2000\n",
      "building tree 1892 of 2000\n",
      "building tree 1893 of 2000\n",
      "building tree 1894 of 2000\n",
      "building tree 1895 of 2000\n",
      "building tree 1896 of 2000\n",
      "building tree 1897 of 2000\n",
      "building tree 1898 of 2000\n",
      "building tree 1899 of 2000\n",
      "building tree 1900 of 2000\n",
      "building tree 1901 of 2000\n",
      "building tree 1902 of 2000\n",
      "building tree 1903 of 2000\n",
      "building tree 1904 of 2000\n",
      "building tree 1905 of 2000\n",
      "building tree 1906 of 2000\n",
      "building tree 1907 of 2000\n",
      "building tree 1908 of 2000\n",
      "building tree 1909 of 2000\n",
      "building tree 1910 of 2000\n",
      "building tree 1911 of 2000\n",
      "building tree 1912 of 2000\n",
      "building tree 1913 of 2000\n",
      "building tree 1914 of 2000\n",
      "building tree 1915 of 2000\n",
      "building tree 1916 of 2000\n",
      "building tree 1917 of 2000\n",
      "building tree 1918 of 2000\n",
      "building tree 1919 of 2000\n",
      "building tree 1920 of 2000\n",
      "building tree 1921 of 2000\n",
      "building tree 1922 of 2000\n",
      "building tree 1923 of 2000\n",
      "building tree 1924 of 2000\n",
      "building tree 1925 of 2000\n",
      "building tree 1926 of 2000\n",
      "building tree 1927 of 2000\n",
      "building tree 1928 of 2000\n",
      "building tree 1929 of 2000\n",
      "building tree 1930 of 2000\n",
      "building tree 1931 of 2000\n",
      "building tree 1932 of 2000\n",
      "building tree 1933 of 2000\n",
      "building tree 1934 of 2000\n",
      "building tree 1935 of 2000\n",
      "building tree 1936 of 2000\n",
      "building tree 1937 of 2000\n",
      "building tree 1938 of 2000\n",
      "building tree 1939 of 2000\n",
      "building tree 1940 of 2000\n",
      "building tree 1941 of 2000\n",
      "building tree 1942 of 2000\n",
      "building tree 1943 of 2000\n",
      "building tree 1944 of 2000\n",
      "building tree 1945 of 2000\n",
      "building tree 1946 of 2000\n",
      "building tree 1947 of 2000\n",
      "building tree 1948 of 2000\n",
      "building tree 1949 of 2000\n",
      "building tree 1950 of 2000\n",
      "building tree 1951 of 2000\n",
      "building tree 1952 of 2000\n",
      "building tree 1953 of 2000\n",
      "building tree 1954 of 2000\n",
      "building tree 1955 of 2000\n",
      "building tree 1956 of 2000\n",
      "building tree 1957 of 2000\n",
      "building tree 1958 of 2000\n",
      "building tree 1959 of 2000\n",
      "building tree 1960 of 2000\n",
      "building tree 1961 of 2000\n",
      "building tree 1962 of 2000\n",
      "building tree 1963 of 2000\n",
      "building tree 1964 of 2000\n",
      "building tree 1965 of 2000\n",
      "building tree 1966 of 2000\n",
      "building tree 1967 of 2000\n",
      "building tree 1968 of 2000\n",
      "building tree 1969 of 2000\n",
      "building tree 1970 of 2000\n",
      "building tree 1971 of 2000\n",
      "building tree 1972 of 2000\n",
      "building tree 1973 of 2000\n",
      "building tree 1974 of 2000\n",
      "building tree 1975 of 2000\n",
      "building tree 1976 of 2000\n",
      "building tree 1977 of 2000\n",
      "building tree 1978 of 2000\n",
      "building tree 1979 of 2000\n",
      "building tree 1980 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 45.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1981 of 2000\n",
      "building tree 1982 of 2000\n",
      "building tree 1983 of 2000\n",
      "building tree 1984 of 2000\n",
      "building tree 1985 of 2000\n",
      "building tree 1986 of 2000\n",
      "building tree 1987 of 2000\n",
      "building tree 1988 of 2000\n",
      "building tree 1989 of 2000\n",
      "building tree 1990 of 2000\n",
      "building tree 1991 of 2000\n",
      "building tree 1992 of 2000\n",
      "building tree 1993 of 2000\n",
      "building tree 1994 of 2000\n",
      "building tree 1995 of 2000\n",
      "building tree 1996 of 2000\n",
      "building tree 1997 of 2000\n",
      "building tree 1998 of 2000\n",
      "building tree 1999 of 2000\n",
      "building tree 2000 of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed: 46.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=13,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=3,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=2000,\n",
       "                      n_jobs=-1, oob_score=False, random_state=42, verbose=2,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators = 2000, n_jobs=-1, min_samples_split = 3, criterion='mse', \n",
    "                      max_features = 'auto', max_depth = 13, random_state=42, verbose=2)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 1977 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=4)]: Done 2000 out of 2000 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14353837916595746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done 1977 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=4)]: Done 2000 out of 2000 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057181342070852646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    4.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997566175695646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 1977 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done 2000 out of 2000 | elapsed:    5.3s finished\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(y_test, rf_model.predict(X_test))))\n",
    "print(np.sqrt(mean_squared_error(y_train, rf_model.predict(X_train))))\n",
    "print(r2_score(y_train, rf_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1436493518680449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0571904713237121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997565398492091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:    3.5s finished\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(y_test, rf_model.predict(X_test))))\n",
    "print(np.sqrt(mean_squared_error(y_train, rf_model.predict(X_train))))\n",
    "print(r2_score(y_train, rf_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 571.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seconds 40461.889\n"
     ]
    }
   ],
   "source": [
    "#WARNING: This takes a very long time to run.\n",
    "start_time = time.time()\n",
    "rf_parameters = {\n",
    "    'max_depth' :[11, 12, 13],\n",
    "    'min_samples_split':[3],\n",
    "    'n_estimators':[1500]\n",
    "}\n",
    "\n",
    "rf_gridsearcher = GridSearchCV(RandomForestRegressor(criterion='mse', max_features = 'auto'), \n",
    "                               rf_parameters, verbose=2, n_jobs=-1, cv=5)\n",
    "rf_gridsearcher.fit(X_train, y_train)\n",
    "print(\" Seconds %0.3f\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 13, 'min_samples_split': 3, 'n_estimators': 1500}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gridsearcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.2, eval_metric='rmse',\n",
    "             gamma=0, importance_type='gain', learning_rate = 0.0625,\n",
    "             max_delta_step=0, max_depth = 5, min_child_weight = 9, missing=None,\n",
    "             n_estimators=1400, n_jobs=-1, nthread=None,\n",
    "             objective='reg:squarederror', random_state=42, reg_alpha=0,\n",
    "             reg_lambda=0, scale_pos_weight=1, seed=None, silent=None,\n",
    "             subsample=1, verbosity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_test = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.5,\n",
    "              importance_type='split', learning_rate=0.1, max_depth=5,\n",
    "              min_child_samples=26, min_child_weight=4, min_split_gain=0.0,\n",
    "              n_estimators=600, n_jobs=1, num_leaves =22,\n",
    "              objective=None, random_state=42, reg_alpha=0.0, reg_lambda=0.0,\n",
    "              silent=True, subsample=1.0, subsample_for_bin=200000,\n",
    "              subsample_freq=0)\n",
    "#lgbm_test.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-abfd0670d3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m stacked_model_final = StackingCVRegressor(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mregressors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxgb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgbm_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmeta_regressor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_test' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "stacked_model_final = StackingCVRegressor(\n",
    "    regressors=[xgb_test, lgbm_test, rf_model, history],\n",
    "    meta_regressor=Ridge()\n",
    ")\n",
    "stacked_model_final.fit(X_train, y_train)\n",
    "print(\" Minutes %0.3f\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=2)]: Done 700 out of 700 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-12.09088433, -14.59697644, -15.29570737, ..., -10.0243968 ,\n",
       "       -11.48273995,  -7.65374434])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-2b9f3bc4d9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "X_test = X_test.as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=2)]: Done 700 out of 700 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "stacked_pred = stacked_model_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07092646627358168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025632786944576016\n",
      "0.9996212091573082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 700 out of 700 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(y_test, stacked_pred)))\n",
    "print(np.sqrt(mean_squared_error(y_train, stacked_model_final.predict(X_train))))\n",
    "print(r2_score(y_test, stacked_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.2, eval_metric='rmse',\n",
    "             gamma=0, importance_type='gain', learning_rate = 0.0625,\n",
    "             max_delta_step=0, max_depth = 5, min_child_weight = 9, missing=None,\n",
    "             n_estimators=1400, n_jobs=-1, nthread=None,\n",
    "             objective='reg:squarederror', random_state=42, reg_alpha=0,\n",
    "             reg_lambda=0, scale_pos_weight=1, seed=None, silent=None,\n",
    "             subsample=1, verbosity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "def data_gen(t=0):\n",
    "    cnt = 0\n",
    "    while cnt < 1000:\n",
    "        cnt += 1\n",
    "        t += 0.1\n",
    "        yield t, np.sin(2*np.pi*t) * np.exp(-t/10.)\n",
    "\n",
    "\n",
    "def init():\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "    ax.set_xlim(0, 10)\n",
    "    del xdata[:]\n",
    "    del ydata[:]\n",
    "    line.set_data(xdata, ydata)\n",
    "    return line,\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line, = ax.plot([], [], lw=2)\n",
    "ax.grid()\n",
    "xdata, ydata = [], []\n",
    "\n",
    "\n",
    "def run(data):\n",
    "    # update the data\n",
    "    t, y = data\n",
    "    xdata.append(t)\n",
    "    ydata.append(y)\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "\n",
    "    if t >= xmax:\n",
    "        ax.set_xlim(xmin, 2*xmax)\n",
    "        ax.figure.canvas.draw()\n",
    "    line.set_data(xdata, ydata)\n",
    "\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, run, data_gen, blit=False, interval=10,\n",
    "                              repeat=False, init_func=init)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_fun(predictions, y_test):\n",
    "    return np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMRegressor(n_jobs=-1)\n",
    "xgb_model = XGBRegressor(n_jobs=-1)\n",
    "rf_model = RandomForestRegressor(n_estimators = 100, min_samples_split = 3, max_features = 'auto', max_depth = 8)\n",
    "knn_model = KNeighborsRegressor(weights = 'distance', n_neighbors = 3, leaf_size = 90)\n",
    "ridge_model = Ridge(alpha = 1000)\n",
    "lasso_model = Lasso(alpha = 0.01, max_iter=10000)\n",
    "enet_model = ElasticNet(alpha = 0.01, l1_ratio = 0.1, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble method for averaging predictions \n",
    "class AveragingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, regressors):\n",
    "        self.regressors = regressors\n",
    "        self.predictions = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for regr in self.regressors:\n",
    "            regr.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.predictions = np.column_stack([regr.predict(X) for regr in self.regressors])\n",
    "        return np.mean(self.predictions, axis=1)\n",
    "    \n",
    "    \n",
    "averaged_model = AveragingRegressor([xgb_model, lgbm_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pred = lgbm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13502644147583798"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, xgb_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09847218775246512"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, lgbm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model = StackingCVRegressor(\n",
    "    regressors=[xgb_model, lgbm_model],\n",
    "    meta_regressor=Ridge()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model.fit(X_train, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = make_scorer(rmse_fun, greater_is_better=False)\n",
    "#let's see how long this all takes\n",
    "start_time = time.time()\n",
    "\n",
    "models = [\n",
    "     ('XGBoost', xgb_model),\n",
    "     ('LightGBM', lgbm_model),\n",
    "     ('RandomForest', rf_model),\n",
    "     ('Ridge', ridge_model),\n",
    "     ('Lasso', lasso_model),\n",
    "     ('ElasticNet', enet_model),\n",
    "     ('KNN', knn_model),\n",
    "     ('Averaged', averaged_model),\n",
    "     ('Stacked', stacked_model),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seconds 11202.304\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = [\n",
    "    -1.0 * cross_val_score(model, X_train.values, y_train.values, scoring=rmse, cv=5, n_jobs=2).mean()\n",
    "    for _,model in models\n",
    "]\n",
    "print(\" Seconds %0.3f\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataz4 = pd.DataFrame({ 'Model': [name for name, _ in models], 'Error (RMSE)': scores })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataz4 = dataz4.append({'Model': 'Neural Net' , 'Error (RMSE)': 0.4261273879608461}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Error (RMSE)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.095604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.196871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.821921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.915745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.903811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.243210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Averaged</td>\n",
       "      <td>0.084234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stacked</td>\n",
       "      <td>0.083640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.426127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Error (RMSE)\n",
       "0       XGBoost      0.087300\n",
       "1      LightGBM      0.095604\n",
       "2  RandomForest      0.196871\n",
       "3         Ridge      0.821921\n",
       "4         Lasso      0.915745\n",
       "5    ElasticNet      0.903811\n",
       "6           KNN      0.243210\n",
       "7      Averaged      0.084234\n",
       "8       Stacked      0.083640\n",
       "9    Neural Net      0.426127"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model Performance')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = dataz4.plot(x='Model', kind='bar', figsize=(8,6))\n",
    "plt.title(\"Model Performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # s is an instance of Series\n",
    "fig = a.get_figure()\n",
    "fig.savefig('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  15 out of  15 | elapsed: 648.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seconds 43667.158\n",
      "Best score and parameter combination = \n",
      "-0.1423195518459505\n",
      "{'rf_model__max_depth': 8, 'rf_model__min_samples_split': 3, 'rf_model__n_estimators': 1500}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('rf_model', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "param_dist = {\"rf_model__max_depth\": [8],\n",
    "              \"rf_model__n_estimators\": [1000, 1500, 2000],\n",
    "              \"rf_model__min_samples_split\": [3],\n",
    "             }\n",
    "\n",
    "CV = GridSearchCV(pipe, param_grid = param_dist, scoring = 'neg_mean_absolute_error', n_jobs= 2, cv=5, verbose=1)\n",
    "CV.fit(X, y)  \n",
    "print(\" Seconds %0.3f\" % (time.time() - start_time))\n",
    "print('Best score and parameter combination = ')\n",
    "\n",
    "print(CV.best_score_)    \n",
    "print(CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators = 1600, min_samples_split = 3, max_features = 'auto', \n",
    "                                 max_depth = None, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 27.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1700 out of 1700 | elapsed: 38.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=1700,\n",
       "                      n_jobs=-1, oob_score=False, random_state=None, verbose=1,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done 1700 out of 1700 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1424460364067801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    3.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05239581000444951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 1700 out of 1700 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(y_test, rf_model.predict(X_test))))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_train, rf_model.predict(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14230119735122027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:    4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052517747265080884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:    5.1s finished\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(y_test, rf_model.predict(X_test))))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_train, rf_model.predict(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model = StackingCVRegressor(\n",
    "    regressors=[xgb_model, lgbm_model, rf_model, knn],\n",
    "    meta_regressor=Ridge(),\n",
    "    n_jobs=4, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed: 86.0min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:  4.9min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed: 139.8min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 27.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed: 35.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingCVRegressor(cv=5,\n",
       "                    meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
       "                                         fit_intercept=True, max_iter=None,\n",
       "                                         normalize=False, random_state=None,\n",
       "                                         solver='auto', tol=0.001),\n",
       "                    n_jobs=4, pre_dispatch='2*n_jobs', random_state=None,\n",
       "                    refit=True,\n",
       "                    regressors=[XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                             colsample_bylevel=1,\n",
       "                                             colsample_bynode=1,\n",
       "                                             colsample_bytree=0.2,\n",
       "                                             eval_metric...\n",
       "                                                      min_samples_split=3,\n",
       "                                                      min_weight_fraction_leaf=0.0,\n",
       "                                                      n_estimators=1600,\n",
       "                                                      n_jobs=-1,\n",
       "                                                      oob_score=False,\n",
       "                                                      random_state=None,\n",
       "                                                      verbose=1,\n",
       "                                                      warm_start=False),\n",
       "                                KNeighborsRegressor(algorithm='auto',\n",
       "                                                    leaf_size=80,\n",
       "                                                    metric='minkowski',\n",
       "                                                    metric_params=None,\n",
       "                                                    n_jobs=3, n_neighbors=3,\n",
       "                                                    p=1, weights='uniform')],\n",
       "                    shuffle=True, store_train_meta_features=False,\n",
       "                    use_features_in_secondary=False, verbose=2)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:   51.9s finished\n"
     ]
    }
   ],
   "source": [
    "stack_pred = stacked_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06724449175609974"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, stack_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stacked_regressor.pkl']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(stacked_model, 'stacked_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07125374765257922"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, stack_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:   17.3s finished\n"
     ]
    }
   ],
   "source": [
    "stacked_train = stacked_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, stacked_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4873, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done 1600 out of 1600 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9996177053252209"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model.store_train_meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
